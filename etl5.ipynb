{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Problem:\n",
    "Algorithm used for extracting subwords did not feel satisfactory.\n",
    "\n",
    "Goal:   Reduce the amount of words in the dictionary by decomposing english into the components (affixes) that give meaning to a word\n",
    "        Target affixes that are most frequently reused and carry meaning consistently\n",
    "        Final product is a dictionary that will be able to break down any word into its components while maintaining information integrity\n",
    "\n",
    "Terms\n",
    "Affix: A word or fragment of a word that carries meaning.\n",
    "    - Words can be composed entirely of affixes, or out of affixes and a root, or exist only as a root\n",
    "    - An affix must carry consistent meaning\n",
    "    - Denoted as a prefix by a underscore on the left or as a suffix by a underscore on the right '_pre' | 'ing_'\n",
    "Parsing: finding all cases of an affix, counting them and extracting from their root word appropriately, then returning the word to the word list\n",
    "     OR: finding a subset of words and identifying and counting all affixes before removing the word from the word list\n",
    "\n",
    "Subgoals:\n",
    "Create a list of affixes that compose the english language from a list of english words\n",
    "Retain as much information as possible while transitioning the word list to the affix list\n",
    "Nested affixes in the affix list should be seperated\n",
    "The final affix list should be able to compose most words in english\n",
    "Words should not be broken down into affixes if it destroys the meaning of the word\n",
    "Words should remain as whole words regardless of length if it cannot be broken down\n",
    "The algorithm should accomplish its goal with very little manual intervention\n",
    "\n",
    "Procedures\n",
    "1) Isolate commonly used words that are affixes to manually parse. Words for orientation are very common (s_ ing_ ed_ est_ er_ | _up _down _over _near _side _under)\n",
    "2) Create rules for affix extraction so that the remaining word will be in the most commonly found state (extract ing_ from _writing_ should yield _write_ not _writ_)\n",
    "3) Manually parse all contractions (words with ' (I'll / it's)) and remove all remaining words with apostrophes\n",
    "4) Create fragments of words by sliding windows of size 2..9 over a word (observing a word fragment) and adding the word count to the tally for that fragment\n",
    "5) Parse occuring more than 3000k times are moved to end dictionary\n",
    "6) Parse occuring more than 100k times with 4 or less letters are moved to end dictionary\n",
    "7) Parse with 3 or less letters are moved to end dictionary\n",
    "8) Create a subset of single character affixes were manually identified but keep them in the affix list\n",
    "9) Remove affixes that have no vowels (other than the single letter affixes) as the do not carry meaning\n",
    "10) Filter out non-affix fragments (no _ indicator) that are length 2 or less\n",
    "11) FIlter out non-affix fragments that do not have an affixed version (mip never occurs at the beginning or end of a word in the vocab and is not likely to be an affix)\n",
    "12) Filter out non-affix fragments when the sum of their affixed versions occur far more often frequently\n",
    "13) Apply affix counting algorithm on the affix list to amplify signal of true affixes\n",
    "14) Iterate through fragments, filtering for fragments that only appear in 2 or less words, and remove from the affix list\n",
    "\n",
    "Problems\n",
    "Identifying the appropriate cutoff point for an affix series 's_', 'es_', 'ities_' all carry meaning and occur uniquely but 'ties_' does not\n",
    "    Cannot cut off all branches after finding a fragment that carries no information\n",
    "Counts are not reliable in all cases. Affixes that carry meaning could occur infrequently but consistently.\n",
    "False extractions occur 't_' is an affix for _burnt_ but not for _beat_. How to discriminate?\n",
    "False extractions can stop the proper extraction from occuring. extracting s_ _from viruses_ leaves _viruse_ which means es_ cannot be extracted\n",
    "Nested ruled affixes. 'ities_' is two affixes. 'ies_' and 'ity_'\n",
    "Uncertainty. is '_a' or '_ab' the affix for '_abbreviation_'\n",
    "Count uncertainty. 'ng_' occurs more than 'ing_' but 'ing_' extracts leaving a coherent root word\n",
    "\n",
    "\n",
    "Tasks\n",
    "Eliminate nested affixes\n",
    "Ruled replacement\n",
    "Affix Tree\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import btk\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AffixAnalyzer:\n",
    "\n",
    "    def __init__(self, words, load=False):\n",
    "        self.vowels = {'a', 'e', 'i', 'o', 'u', 'y'}\n",
    "        self.singles = {'s_', 'd_', 'r_', 'n_', 't_', 'x_', 'y_', 'a_', 'i_', 'o_', '_a', '_o', '_e', '_i'}\n",
    "        self.doubles = {'b', 'c', 'd', 'f', 'g', 'l', 'm', 'n', 'p', 'r', 's', 't'}\n",
    "        self.cleared = Counter()\n",
    "        self.frags = Counter()\n",
    "        self.final = Counter()\n",
    "        if load:\n",
    "            self.load()\n",
    "        else:\n",
    "            self.wlst = Counter({f'_{x[1]}_': int(x[0]) for x in words[::-1]})\n",
    "            self.prep_frags()\n",
    "            self.clean_contractions()\n",
    "            self.create_frags()\n",
    "            self.clean_common()\n",
    "\n",
    "    def wcnt_to_occnt(self, wcnt):\n",
    "        return round(np.log2(wcnt), 3) ** 3\n",
    "\n",
    "    def get_afxline(self, afx, counts=False):\n",
    "        if counts:\n",
    "            return [x for x in self.frags.most_common() if afx in x[0] or x[0] in afx]\n",
    "        else:\n",
    "            return [x[0] for x in self.frags.most_common() if afx in x[0] or x[0] in afx]\n",
    "\n",
    "    def get_afxwords(self, afx, counts=False):\n",
    "        if counts:\n",
    "            return [x for x in self.wlst.most_common() if afx in x[0]]\n",
    "        else:\n",
    "            return {x[0] for x in self.wlst.most_common() if afx in x[0]}\n",
    "\n",
    "    def get_afx(self, afx, counts=False):\n",
    "        if counts:\n",
    "            return [x for x in self.frags.most_common() if afx in x[0]]\n",
    "        else:\n",
    "            return {x[0] for x in self.frags.most_common() if afx in x[0]}\n",
    "\n",
    "    def get_rep(self, afx, word):\n",
    "        word = word.replace(afx, '_')\n",
    "        if afx[0] == '_':\n",
    "            if (word[1] == word[2] or word[1] == afx[-1]) and word[1] in self.doubles:\n",
    "                word = f'_{word[2:]}'\n",
    "        else:\n",
    "            if (word[-2] == word[-3] or word[-2] == afx[0]) and word[-2] in self.doubles:\n",
    "                word = f'{word[:-2]}_'\n",
    "        if word in self.wlst or word in self.cleared or word in self.final:\n",
    "            return True\n",
    "\n",
    "    def seek_tree(self, keys):\n",
    "        for x in keys:\n",
    "            target = self.tree[x]\n",
    "        return target\n",
    "\n",
    "    def degrade(self, afx):\n",
    "        if afx[0] == '_':\n",
    "            return afx[1:]\n",
    "        elif afx[-1] == '_':\n",
    "            return afx[:-1]\n",
    "\n",
    "    def clean_common(self):\n",
    "        for x in self.wlst.most_common():\n",
    "            if x[1] > 3000000:\n",
    "                self.cleared[x[0]] = x[1]\n",
    "            elif x[1] > 100000 and len(x[0]) < 6:\n",
    "                self.cleared[x[0]] = x[1]\n",
    "            elif x[1] < 100000:\n",
    "                break\n",
    "        for z in [y for y in self.wlst if len(y) < 6]:\n",
    "            self.cleared[z] += self.wlst[z]\n",
    "        for z in [x for x in self.frags if not any(y in x for y in self.vowels) and x not in self.singles]:\n",
    "            self.frags.pop(z)\n",
    "        for z in [y for y in self.frags if len(y) == 2 and y not in self.singles]:\n",
    "            self.frags.pop(z)\n",
    "        for z in {x for x in self.frags if '_' not in x and (f'{x}_' in self.frags or f'_{x}' in self.frags) and (f'_{x}_' in self.cleared or f'_{x}_' in self.final or f'_{x}_' in self.wlst)}:\n",
    "            self.frags.pop(z)\n",
    "        for z in {x for x in self.frags if '_' not in x and (self.frags[f'_{x}'] + self.frags[f'{x}_']) / self.frags[x] > 0.666}:\n",
    "            self.frags.pop(z)\n",
    "        for z in self.cleared:\n",
    "            if z in self.wlst:\n",
    "                self.wlst.pop(z)\n",
    "        for _ in range(2):\n",
    "            for word in self.frags:\n",
    "                tmplen = min(len(word), 10)\n",
    "                for n in range(2, tmplen+1):\n",
    "                    for pos in range(tmplen-n+2):\n",
    "                        if word[pos:pos+n] in self.frags:\n",
    "                            self.frags[word[pos:pos+n]] += self.frags[word]\n",
    "        solos = []\n",
    "        for afx in self.frags:\n",
    "            c = 0\n",
    "            for w in self.wlst:\n",
    "                if afx in w:\n",
    "                    c += 1\n",
    "                if c > 2:\n",
    "                    break\n",
    "            else:\n",
    "                solos.append(afx)\n",
    "        for x in solos:\n",
    "            self.frags.pop(x)\n",
    "        self.frags = Counter({x[0]: int(x[1]**(1/np.e)) for x in self.frags.most_common()})\n",
    "        self.frags['_'] = 10000\n",
    "\n",
    "    def clean_contractions(self):\n",
    "        for x in [\"_ain't_\", \"_can't_\", \"_won't_\", \"_shan't_\"]:\n",
    "            self.final[\"n't_\"] += self.wlst[x]\n",
    "        for x in [\"_i'm_\", \"_can't_\"]:\n",
    "            self.wlst[f'{x[:-3]}_'] += self.wlst[x]\n",
    "        for x in [\"_ma'am_\", \"_ain't_\", \"_i'm_\"]:\n",
    "            self.final[x] += self.wlst[x]\n",
    "        for x in [\"_ain't_\", \"_can't_\", \"_won't_\", \"_shan't_\", \"_ma'am_\", \"_i'm_\", \"_van't_\"]:\n",
    "            self.wlst.pop(x)\n",
    "        for x in [z for z in self.wlst if \"'\" in z if any(y in z for y in [\"'s_\", \"'ll_\", \"'ve_\", \"n't_\", \"'re_\", \"'d_\"])]:\n",
    "            for efx in [\"'s_\", \"'ll_\", \"'ve_\", \"n't_\", \"'re_\", \"'d_\"]:\n",
    "                if efx in x:\n",
    "                    self.final[efx] += self.wlst[x]\n",
    "                    self.wlst[x.replace(efx, '_')] += self.wlst[x]\n",
    "                    self.wlst.pop(x)\n",
    "        for x in [z for z in self.wlst if \"'\" in z]:\n",
    "            self.wlst.pop(x)\n",
    "\n",
    "    def prep_frags(self):\n",
    "        with open(r'D:\\dstore\\nlp\\w2v\\directions', 'rb') as f:\n",
    "            directions = pickle.load(f)\n",
    "        self.frags['_a'] += self.wlst[\"_around_\"]\n",
    "        self.frags[\"_round\"] += self.wlst[\"_around_\"]\n",
    "        self.frags[\"_o\"] += self.wlst[\"_over_\"]\n",
    "        self.frags[\"_ver\"] += self.wlst[\"_over_\"]\n",
    "        for x in directions[0]:\n",
    "            self.frags[x[:-1]] += self.wlst[x]\n",
    "        for x in directions[2]:\n",
    "            self.frags[x[:-1]] += self.wlst[x]\n",
    "            self.frags[x[1:]] += self.wlst[x]\n",
    "        for x in directions[1]:\n",
    "            self.frags[x[1:]] += self.wlst[x]\n",
    "        for y in directions:\n",
    "            for x in y:\n",
    "                if len(x) < 5:\n",
    "                    self.final[x] += self.wlst[x]\n",
    "                    self.wlst.pop(x)\n",
    "                else:\n",
    "                    self.cleared[x] += self.wlst[x]\n",
    "                    self.wlst.pop(x)\n",
    "\n",
    "    def create_frags(self):\n",
    "        for word in self.wlst:\n",
    "            tmplen = min(len(word), 10)\n",
    "            for n in range(2, tmplen-1):\n",
    "                for pos in range(tmplen-n+1):\n",
    "                    self.frags[word[pos:pos+n]] += self.wlst[word]\n",
    "        for x in {y for y in self.frags if '_' in y and len(y) == 2}:\n",
    "            if x not in self.singles:\n",
    "                self.frags.pop(x)\n",
    "        for x in [y for y in self.frags if '_' not in y]:\n",
    "            if f'_{x}' not in self.frags and f'{x}_' not in self.frags:\n",
    "                self.frags.pop(x)\n",
    "        tmp = [x[0] for x in self.frags.most_common()]\n",
    "        for i, x in enumerate(tmp):\n",
    "            lim = 0\n",
    "            while lim < 2048:\n",
    "                try:\n",
    "                    if x in tmp[i+1+lim]:\n",
    "                        self.frags[x] -= self.frags[tmp[i+1+lim]]\n",
    "                        break\n",
    "                except IndexError:\n",
    "                    break\n",
    "                lim += 1\n",
    "        self.frags = Counter({x[0]: x[1] for x in self.frags.most_common() if x[1] > 128})\n",
    "\n",
    "    def sieve_level(self, inp):\n",
    "        out = []\n",
    "        i = min(len(x) for x in inp)\n",
    "        while inp or i <= 8:\n",
    "            active = {x for x in inp if len(x) == i}\n",
    "            for x in active:\n",
    "                tmp = {y for y in inp if x in y}\n",
    "                for y in tmp:\n",
    "                    inp.remove(y)\n",
    "                out.append(x)\n",
    "            i += 1\n",
    "        return out\n",
    "\n",
    "    def af_tree(self):\n",
    "        longest = max({len(x) for x in self.frags if '_' in x})\n",
    "        self.tree = {'_': {x: None for x in self.frags if '_' in x and len(x) == 2}}\n",
    "        targets = {x for x in self.frags if any(y in x for y in self.tree['_'])}\n",
    "        for x in self.tree['_']: targets.remove(x)\n",
    "        tracker = ['_']\n",
    "        \n",
    "        while targets:\n",
    "            pass\n",
    "\n",
    "    def load(self):\n",
    "        with open(r'C:\\Users\\BBA\\Coding\\NLP\\Embeddings\\data\\words', 'rb') as f:\n",
    "            self.wlst = pickle.load(f)\n",
    "        with open(r'C:\\Users\\BBA\\Coding\\NLP\\Embeddings\\data\\frags', 'rb') as f:\n",
    "            self.frags = pickle.load(f)\n",
    "        with open(r'C:\\Users\\BBA\\Coding\\NLP\\Embeddings\\data\\cleared', 'rb') as f:\n",
    "            self.cleared = pickle.load(f)\n",
    "        with open(r'C:\\Users\\BBA\\Coding\\NLP\\Embeddings\\data\\final', 'rb') as f:\n",
    "            self.final = pickle.load(f)\n",
    "\n",
    "    def save(self):\n",
    "        with open(r'C:\\Users\\BBA\\Coding\\NLP\\Embeddings\\data\\words', 'wb') as f:\n",
    "            pickle.dump(self.wlst, f)\n",
    "        with open(r'C:\\Users\\BBA\\Coding\\NLP\\Embeddings\\data\\frags', 'wb') as f:\n",
    "            pickle.dump(self.frags, f)\n",
    "        with open(r'C:\\Users\\BBA\\Coding\\NLP\\Embeddings\\data\\cleared', 'wb') as f:\n",
    "            pickle.dump(self.cleared, f)\n",
    "        with open(r'C:\\Users\\BBA\\Coding\\NLP\\Embeddings\\data\\final', 'wb') as f:\n",
    "            pickle.dump(self.final, f)\n",
    "\n",
    "with open(r'D:\\dstore\\nlp\\w2v\\fwords', 'rt') as f:\n",
    "    afa = AffixAnalyzer([x.strip().split() for x in f.readlines()], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\dstore\\nlp\\w2v\\word_nests2', 'rb') as f:\n",
    "    solos, multi, afx_cand = pickle.load(f)\n",
    "\n",
    "with open(r'D:\\dstore\\nlp\\w2v\\word_nests2', 'wb') as f:\n",
    "    pickle.dump((solos, multi), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'ated_'\n",
    "keys = [test[i:] for i, x in enumerate(test)][::-1][1:-1]\n",
    "tdct = {'_': {'d_': {'ed_': {'ted_': {'ated_': False}}}}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a031010b1865a23c8ecad84a31bb65ec3bf0e64be1ddea340d48586579fa20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
