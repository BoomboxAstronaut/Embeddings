{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create subword dictionary to breakdown words for embedding training\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Assets\n",
    "\n",
    "with open(r'D:\\dstore\\nlp\\w2v\\wcounts', 'rt', encoding='utf8') as f:\n",
    "    wlst = [x.strip().split() for x in f.readlines()]\n",
    "\n",
    "#Filter out words w/ < 256 occurences \n",
    "wlst = Counter({x[1]: int(x[0]) for x in wlst})\n",
    "\n",
    "#Count letters in all words\n",
    "lcount = Counter()\n",
    "for x in wlst:\n",
    "    for y in x:\n",
    "        lcount[y] += 1\n",
    "\n",
    "#Filter out letters w/ < 256 occurences\n",
    "lcount = {x[0]: x[1] for x in lcount.items() if x[1] >= 256}\n",
    "alpha = [*lcount, '_', \"'\", '[UNK]']\n",
    "wlst = Counter({f\"_{' '.join(list(x[0]))}_\": x[1] for x in wlst.items()})\n",
    "\n",
    "stubs = {f'{x[:-3]}_' for x in wlst}\n",
    "full_words = [''.join(x[0].split()) for x in wlst.most_common() if x[1] > 1000000 and x[0] not in ['_u s e d_', '_u n i t e d_', '_i n c l u d i n g_', '_r e l e a s e d_', '_c a l l e d_', '_f o l l o w i n g_', '_b a s e d_', '_p l a y e d_', '_l o c a t e d_']]\n",
    "full_words.extend(['_i_', \"_i'm_\", '_a_'])\n",
    "wlst = Counter({x[0]: x[1] for x in wlst.items() if ''.join(x[0].split()) not in full_words})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precombine common suffix subwords first to precondition byte encoding\n",
    "primes = set()\n",
    "\n",
    "with open(r'D:\\dstore\\nlp\\w2v\\prefs2', 'rt', encoding='utf8') as f:\n",
    "    for x in f.readlines():\n",
    "        primes.add(x.strip())\n",
    "\n",
    "with open(r'D:\\dstore\\nlp\\w2v\\suffs2', 'rt', encoding='utf8') as f:\n",
    "    for x in f.readlines():\n",
    "        primes.add(x.strip())\n",
    "\n",
    "isubwords = [('s_', 'es_'), ('r_', 'er_', 'or_'), ('d_', 'ed_')]\n",
    "\n",
    "primes = sorted(primes, key=lambda x: len(x), reverse=True)\n",
    "mods = []\n",
    "for word in wlst:\n",
    "    temp = []\n",
    "    for subw in primes:\n",
    "        esubw = ' '.join(list(subw)).replace(' _', '_').replace('_ ', '_')\n",
    "        if esubw[0] == '_':\n",
    "            rep = word.replace(f'{esubw} ', '_')\n",
    "            if esubw in word and rep != word and rep in wlst:\n",
    "                temp.append(esubw)\n",
    "        elif esubw[-1] == '_':\n",
    "            rep = word.replace(f' {esubw}', '_')\n",
    "            if esubw in word and rep != word and (rep in wlst or rep in stubs):\n",
    "                temp.append(esubw)\n",
    "    for group in isubwords:\n",
    "        for subw in group:\n",
    "            esubw = ' '.join(list(subw)).replace(' _', '_')\n",
    "            rep = word.replace(f' {esubw}', '_')\n",
    "            if esubw in word and rep != word and rep in wlst and esubw not in temp:\n",
    "                temp.append(esubw)\n",
    "                break\n",
    "    if len(temp) > 0:\n",
    "        for x in temp[::-1]:\n",
    "            for y in temp:\n",
    "                if x != y and x in y:\n",
    "                    temp.remove(x)\n",
    "                    break\n",
    "        mods.append((word, temp))\n",
    "\n",
    "for x in mods:\n",
    "    new_word = x[0]\n",
    "    for y in x[1]:\n",
    "        merged = ''.join(y.split())\n",
    "        new_word = new_word.replace(y, merged)\n",
    "    if new_word != x[0]:\n",
    "        wlst[new_word] += wlst[x[0]]\n",
    "        wlst.pop(x[0])\n",
    "\n",
    "subwords = set()\n",
    "\n",
    "with open(r'D:\\dstore\\nlp\\w2v\\prefs', 'rt', encoding='utf8') as f:\n",
    "    for x in f.readlines():\n",
    "        subwords.add(x.strip())\n",
    "\n",
    "with open(r'D:\\dstore\\nlp\\w2v\\suffs', 'rt', encoding='utf8') as f:\n",
    "    for x in f.readlines():\n",
    "        subwords.add(x.strip())\n",
    "\n",
    "subwords = sorted(subwords, key=lambda x: len(x), reverse=True)\n",
    "mods = []\n",
    "for word in wlst:\n",
    "    temp = []\n",
    "    for subw in subwords:\n",
    "        esubw = ' '.join(list(subw)).replace(' _', '_').replace('_ ', '_')\n",
    "        if esubw[0] == '_':\n",
    "            rep = word.replace(f'{esubw} ', '_')\n",
    "            if esubw in word and rep != word and rep in wlst:\n",
    "                temp.append(esubw)\n",
    "        elif esubw[-1] == '_':\n",
    "            rep = word.replace(f' {esubw}', '_')\n",
    "            if esubw in word and rep != word and (rep in wlst or rep in stubs):\n",
    "                temp.append(esubw)\n",
    "    if len(temp) > 0:\n",
    "        for x in temp[::-1]:\n",
    "            for y in temp:\n",
    "                if x != y and x in y:\n",
    "                    temp.remove(x)\n",
    "                    break\n",
    "        mods.append((word, temp))\n",
    "\n",
    "for x in mods:\n",
    "    new_word = x[0]\n",
    "    for y in x[1]:\n",
    "        merged = ''.join(y.split())\n",
    "        new_word = new_word.replace(y, merged)\n",
    "    if new_word != x[0]:\n",
    "        wlst[new_word] += wlst[x[0]]\n",
    "        wlst.pop(x[0])\n",
    "\n",
    "subwords.extend(primes)\n",
    "\n",
    "#Remove fully merged words from word list\n",
    "temp = []\n",
    "for x in wlst:\n",
    "    if ' ' not in x:\n",
    "        temp.append(x)\n",
    "for x in temp:\n",
    "    full_words.append(x)\n",
    "    wlst.pop(x)\n",
    "\n",
    "subwords = set(subwords)\n",
    "\n",
    "del temp, lcount, mods, isubwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Counts pairs of subwords\n",
    "candidate_frags = Counter()\n",
    "for word in wlst:\n",
    "    word = word.split()\n",
    "    for i, _ in enumerate(word[:-1]):\n",
    "        candidate_frags[f'{word[i]} {word[i+1]}'] += wlst[' '.join(word)]\n",
    "\n",
    "candidate_frags = Counter({x[0]: x[1] for x in candidate_frags.items()})\n",
    "\n",
    "#Byte encoding algorithm\n",
    "#Processes the 16 most common pairs, counts pairs repeats\n",
    "#Check if subword occurs for any word in word_list.\n",
    "#If subword pair is found, merge pair and replace the pair with the combined subword\n",
    "\n",
    "while len(subwords) < 20000 or len(wlst) > 0:\n",
    "    count = 0\n",
    "    while count < 16:\n",
    "        sub = candidate_frags.most_common()[0][0]\n",
    "        matches = []\n",
    "        csub = sub.split()\n",
    "        for word in wlst:\n",
    "            if sub not in word:\n",
    "                continue\n",
    "            if f'{sub} ' != word[:len(sub) + 1] and f' {sub}' != word[len(word) - len(sub) - 1:] and f' {sub} ' not in word and len(sub) != len(word):\n",
    "                continue\n",
    "            word = word.split()\n",
    "            is_match = False\n",
    "            idx = []\n",
    "            outp = word.copy()\n",
    "            for i, y in enumerate(word[:-1]):\n",
    "                if is_match:\n",
    "                    is_match = False\n",
    "                    continue\n",
    "                if csub[0] == y and csub[1] == word[i+1]:\n",
    "                    idx.append(i)\n",
    "                    is_match = True\n",
    "            for i, pos in enumerate(idx):\n",
    "                outp.insert(pos + i, ''.join(csub))\n",
    "            for i, pos in enumerate(idx):\n",
    "                outp.pop(pos + 1 - i)\n",
    "                outp.pop(pos + 1 - i)\n",
    "            matches.append((' '.join(word), ' '.join(outp)))\n",
    "        for m in matches:\n",
    "            wlst[m[1]] += wlst[m[0]]\n",
    "            wlst.pop(m[0])\n",
    "        candidate_frags.pop(sub)\n",
    "        sub = sub.replace(' ', '')\n",
    "        if sub[0] == '_' and sub[-1] == '_':\n",
    "            full_words.append(sub)\n",
    "        else:\n",
    "            subwords.add(sub)\n",
    "        count += 1\n",
    "\n",
    "    #Remove fully combined words from list\n",
    "    for x in list(wlst):\n",
    "        if ' ' not in x:\n",
    "            wlst.pop(x)\n",
    "            full_words.append(x)\n",
    "\n",
    "    #Count pairs\n",
    "    candidate_frags = Counter()\n",
    "    for word in wlst:\n",
    "        word = word.split()\n",
    "        for i, _ in enumerate(word[:-1]):\n",
    "            candidate_frags[f'{word[i]} {word[i+1]}'] += wlst[' '.join(word)]\n",
    "    candidate_frags = Counter({x[0]: x[1] for x in candidate_frags.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('_eisted df', 2355)\n",
      "('_fluctu ating_', 2354)\n",
      "('_iso lates_', 2354)\n",
      "('_pedi at', 2354)\n",
      "('_guan aj', 2354)\n",
      "('aj u', 2354)\n",
      "('u ato_', 2354)\n",
      "('_aneurys m_', 2354)\n",
      "('_facsim ile_', 2354)\n",
      "('_solom ons_', 2354)\n",
      "140958 \n",
      "\n",
      "('_fluctu ating_', 2354)\n",
      "('_iso lates_', 2354)\n",
      "('_guan aj u ato_', 2354)\n",
      "('_aneurys m_', 2354)\n",
      "('_facsim ile_', 2354)\n",
      "('_solom ons_', 2354)\n",
      "('_lor en_', 2353)\n",
      "('_fro ze_', 2353)\n",
      "('_roman ization_', 2353)\n",
      "('_south gate_', 2353)\n",
      "119910 \n",
      "\n",
      "20002\n",
      "61576\n"
     ]
    }
   ],
   "source": [
    "for x in candidate_frags.most_common()[:10]: print(x)\n",
    "print(len(candidate_frags), '\\n')\n",
    "for x in wlst.most_common()[:10]: print(x)\n",
    "print(len(wlst), '\\n')\n",
    "\n",
    "print(len(subwords))\n",
    "print(len(full_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'D:/dstore/nlp/w2v/subw-3', 'rb') as f:\n",
    "    subwords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" with open(f'D:/dstore/nlp/w2v/subw-{version}', 'wb') as f:\\n    pickle.dump(subwords, f) \""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "version = 'x5'\n",
    "\n",
    "with open(f'D:/dstore/nlp/w2v/wlst-{version}', 'wb') as f:\n",
    "    pickle.dump(wlst, f)\n",
    "\n",
    "with open(f'D:/dstore/nlp/w2v/subw-{version}', 'wb') as f:\n",
    "    pickle.dump(subwords, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'x5'\n",
    "with open(f'D:/dstore/nlp/w2v/wlst-{version}', 'rb') as f:\n",
    "    wlst = pickle.load(f)\n",
    "\n",
    "\"\"\" with open(f'D:/dstore/nlp/w2v/subw-{version}', 'rb') as f:\n",
    "    subwords = pickle.load(f) \"\"\"\n",
    "\n",
    "with open(f'D:/dstore/nlp/w2v/pwlst', 'rb') as f:\n",
    "    subwords = pickle.load(f)\n",
    "\n",
    "with open(f'D:/dstore/nlp/w2v/cwlst', 'rb') as f:\n",
    "    cwords = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcounter = Counter()\n",
    "for x in subwords:\n",
    "    for y in twords:\n",
    "        if x in y:\n",
    "            subcounter[x] += 1\n",
    "\n",
    "tcounter = Counter({x[0]: x[1] for x in subcounter.most_common() if (len(x[0]) > 2 or '_' in x[0]) and len(x[0]) < 8})\n",
    "test = tcounter.most_common()\n",
    "tlen = len(test)\n",
    "\n",
    "for i, x in enumerate(test):\n",
    "    for j in range(1, tlen-i):\n",
    "        if x[0] in test[i+j][0]:\n",
    "            tcounter[x[0]] -= tcounter[test[i+j][0]]\n",
    "            break\n",
    "\n",
    "test = tcounter.most_common()\n",
    "tlen = len(test)\n",
    "\n",
    "for i, x in enumerate(test):\n",
    "    j = 1\n",
    "    avg_group = []\n",
    "    limiter = 0\n",
    "    ulim = 0\n",
    "    while len(avg_group) < 8:\n",
    "        if i+j < tlen:\n",
    "            if len(test[i+j][0]) == len(x[0]):\n",
    "                avg_group.append(test[i+j][1])\n",
    "        if i-j >= 0 and ulim < 3:\n",
    "            if len(test[i-j][0]) == len(x[0]):\n",
    "                avg_group.append(test[i-j][1])\n",
    "                ulim += 1\n",
    "        j += 1\n",
    "        limiter += 1\n",
    "        if limiter > 4096:\n",
    "            break\n",
    "    if len(avg_group) > 0:\n",
    "        tcounter[x[0]] -= int((np.mean(avg_group) + np.median(avg_group)) / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [x for x in tcounter.most_common() if x[1] > 0]\n",
    "test.pop(0)\n",
    "\n",
    "with open(f'D:/dstore/nlp/w2v/pwlst', 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a031010b1865a23c8ecad84a31bb65ec3bf0e64be1ddea340d48586579fa20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
