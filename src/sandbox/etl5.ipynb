{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem:\n",
    "    Algorithm used for extracting subwords did not feel satisfactory.\n",
    "\n",
    "Goal:\n",
    "    Reduce the amount of words in the dictionary by decomposing english into the components (affixes) that give meaning to a word\n",
    "    Target affixes that are most frequently reused and carry meaning consistently\n",
    "    Final product is a dictionary that will be able to break down any word into its components while maintaining information integrity\n",
    "\n",
    "Terms\n",
    "Affix: \n",
    "    A word or fragment of a word that carries meaning.\n",
    "        - Words can be composed entirely of affixes, or out of affixes and a root, or exist only as a root\n",
    "        - An affix must carry consistent meaning\n",
    "        - Denoted as a prefix by a underscore on the left or as a suffix by a underscore on the right '_pre' | 'ing_'\n",
    "Parsing: \n",
    "        finding all cases of an affix, counting them and extracting from their root word appropriately, then returning the word to the word list\n",
    "    OR: finding a subset of words and identifying and counting all affixes before removing the word from the word list\n",
    "\n",
    "Subgoals:\n",
    "    Create a list of affixes that compose the english language from a list of english words\n",
    "    Retain as much information as possible while transitioning the word list to the affix list\n",
    "    Nested affixes in the affix list should be seperated\n",
    "    The final affix list should be able to compose most words in english\n",
    "    Words should not be broken down into affixes if it destroys the meaning of the word\n",
    "    Words should remain as whole words regardless of length if it cannot be broken down\n",
    "    The algorithm should accomplish its goal with very little manual intervention\n",
    "\n",
    "Procedures:\n",
    "    1) Isolate commonly used words that are affixes to manually parse. Words for orientation are very common (s_ ing_ ed_ est_ er_ | _up _down _over _near _side _under)\n",
    "    2) Create rules for affix extraction so that the remaining word will be in the most commonly found state (extract ing_ from _writing_ should yield _write_ not _writ_)\n",
    "    3) Manually parse all contractions (words with ' (I'll / it's)) and remove all remaining words with apostrophes\n",
    "    4) Create fragments of words by sliding windows of size 2..9 over a word (observing a word fragment) and adding the word count to the tally for that fragment\n",
    "    5) Parse occuring more than 3000k times are moved to end dictionary\n",
    "    6) Parse occuring more than 100k times with 4 or less letters are moved to end dictionary\n",
    "    7) Parse with 3 or less letters are moved to end dictionary\n",
    "    8) Create a subset of single character affixes were manually identified but keep them in the affix list\n",
    "    9) Remove affixes that have no vowels (other than the single letter affixes) as the do not carry meaning\n",
    "    10) Filter out non-affix fragments (no _ indicator) that are length 2 or less\n",
    "    11) FIlter out non-affix fragments that do not have an affixed version (mip never occurs at the beginning or end of a word in the vocab and is not likely to be an affix)\n",
    "    12) Filter out non-affix fragments when the sum of their affixed versions occur far more often frequently\n",
    "    13) Apply affix counting algorithm on the affix list to amplify signal of true affixes\n",
    "    14) Iterate through fragments, filtering for fragments that only appear in 2 or less words, and remove from the affix list\n",
    "    15) SVD:\n",
    "            Create a matrix where each column represents an affix and each row represents a word.\n",
    "            Each element will be binary. 1 indicating that a affix is present in a word, 0 indicating not present\n",
    "\n",
    "Problems:\n",
    "    Identifying the appropriate cutoff point for an affix series 's_', 'es_', 'ities_' all carry meaning and occur uniquely but 'ties_' does not\n",
    "        Cannot cut off all branches after finding a fragment that carries no information\n",
    "    Counts are not reliable in all cases. Affixes that carry meaning could occur infrequently but consistently.\n",
    "    False extractions occur 't_' is an affix for _burnt_ but not for _beat_. How to discriminate?\n",
    "    False extractions can stop the proper extraction from occuring. extracting s_ _from viruses_ leaves _viruse_ which means es_ cannot be extracted\n",
    "    Nested ruled affixes. 'ities_' is two affixes. 'ies_' and 'ity_'\n",
    "    Uncertainty. is '_a' or '_ab' the affix for '_abbreviation_'\n",
    "    Count uncertainty. 'ng_' occurs more than 'ing_' but 'ing_' extracts leaving a coherent root word\n",
    "\n",
    "Tasks\n",
    "Eliminate nested affixes\n",
    "Ruled replacement\n",
    "Affix Tree\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pickle import load, dump\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable, Container, Protocol\n",
    "from btk import cprint, fzip, rdx_sort, hprint, lrsort, rrsort\n",
    "\n",
    "plt.style.use(f\"{os.environ['style']}\")\n",
    "\n",
    "def setup():\n",
    "\n",
    "    with open(r'D:\\dstore\\nlp\\w2v\\fwords', 'rt') as f:\n",
    "        a = AffixAnalyzer([x.strip().split() for x in f.readlines()], 3)\n",
    "    \n",
    "    for x in 'id ax ox ab op ex by on to in'.split():\n",
    "        a.wlst[f'_{x}_'] = a.cleared[f'_{x}_'] \n",
    "    \"\"\" for x in [x for x in a.cleared.most_common() if len(x[0]) > 3 and x[1] < 20500000 and x[1] > 100000]: a.wlst[x[0]] = x[1]\n",
    "    with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\manu{int(2)}', 'rb') as f:\n",
    "        wrd_q, a.wlst, a.roots, a.afxscore, a.wparts, a.failed_brk = load(f) \"\"\"\n",
    "\n",
    "    with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\awork4', 'rb') as f:\n",
    "        roots, nfx = load(f)\n",
    "\n",
    "    with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\nroots', 'rt') as f:\n",
    "        roots = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\rmls', 'rt') as f:\n",
    "        #sorted([(x, a.full_scores[f'_{x}_']) for x in rmls if f'_{x}_' in a.full_scores], key=lambda x: x[1])[::-1]\n",
    "        rmls = [x.strip() for x in f.readlines()]\n",
    "        for x in rmls:\n",
    "            if f'_{x}_' in a.wlst: a.wlst.pop(f'_{x}_')\n",
    "\n",
    "    with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\als', 'rt') as f:\n",
    "        als = [f'_{x.strip()}_' for x in f.readlines()]\n",
    "        for x in als:\n",
    "            if x not in a.wlst: a.wlst[x] = 100\n",
    "            else: print(x, a.wlst[x])\n",
    "    \n",
    "    return a, roots, nfx\n",
    "\n",
    "def merger(results):\n",
    "    r1o, r2o, c = [], [], 0\n",
    "    results = tuple(sorted([(x[0], x[1]) for x in results], key=lambda x: len(x[1]), reverse=True))\n",
    "    while c < len(results[0][1]):\n",
    "        r1, r2 = [], []\n",
    "        for x in results:\n",
    "            if c < len(x[1]):\n",
    "                if x[1][c] not in r2: r2.append(x[1][c])\n",
    "            if c < len(x[1])-1:\n",
    "                if x[0][c] not in r1: r1.append(x[0][c])\n",
    "        if r2:\n",
    "            if len(r2) > 1: r2o.append(tuple(r2))\n",
    "            else:  r2o.append(r2[0])\n",
    "        if r1:\n",
    "            if len(r1) > 1: r1o.append(tuple(r1))\n",
    "            else: r1o.append(r1[0])\n",
    "        c += 1\n",
    "    return (r1o[::-1], r2o[::-1])\n",
    "\n",
    "def packer(results, idx, cid=None):\n",
    "    while cid:\n",
    "        if cid[0] == '1':\n",
    "            if len(cid) < 2 or int(cid[1]) >= len(results): return False\n",
    "            results = results[int(cid[1])]\n",
    "            cid = cid[2:]\n",
    "            if cid: cid = f'2{cid}'\n",
    "        elif cid[0] == '2':\n",
    "            if len(cid) < 2 or int(cid[1]) > len(idx): return False\n",
    "            idx = idx[int(cid[1]):]\n",
    "            cid = cid[2:]\n",
    "            if cid and isinstance(idx[0], tuple) and cid[0] in ('0', '1'):\n",
    "                idx[0] = idx[0][int(cid[0])]\n",
    "                cid = ''\n",
    "        else: cid = ''\n",
    "    out = {}\n",
    "    if isinstance(idx[0], tuple):\n",
    "        print('roots must be a word not a tuple of words')\n",
    "        return False\n",
    "    else:\n",
    "        results = [(x[0][:len(idx)-1], x[1][:len(idx)]) for x in results if idx[0] in x[1]]\n",
    "        for x in results:\n",
    "            for j, y in enumerate(x[1][:-1]):\n",
    "                if y not in out:\n",
    "                    out[y] = [idx[0], []]\n",
    "                    for z in x[0][j:]:\n",
    "                        out[y][1].append(z)\n",
    "    return out\n",
    "\n",
    "def usk_rep(wlst):\n",
    "    with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\ukdct', 'rb') as f:\n",
    "        ouw, aew, rew, oew = load(f)\n",
    "    wid = [x for x in wlst]\n",
    "    for x in wid:\n",
    "        if any(y in x for y in ouw) and x.replace('ou', 'o') in wlst:\n",
    "            wlst[x.replace('ou', 'o')] += wlst[x]\n",
    "            wlst.pop(x)\n",
    "        \"\"\" if any(y in x for y in aew) and x.replace('ae', 'e') in wlst:\n",
    "            wlst[x.replace('ae', 'e')] += wlst[x]\n",
    "            wlst.pop(x) \"\"\"\n",
    "        if any(y in x for y in rew) and x.replace('re', 'er') in wlst:\n",
    "            wlst[x.replace('re', 'er')] += wlst[x]\n",
    "            wlst.pop(x)\n",
    "        if any(y in x for y in oew) and x.replace('oe', 'e') in wlst:\n",
    "            wlst[x.replace('oe', 'e')] += wlst[x]\n",
    "            wlst.pop(x)\n",
    "        if 'ise' in x and x.replace('ise', 'ize') in wlst:\n",
    "            wlst[x.replace('ise', 'ize')] += wlst[x]\n",
    "            wlst.pop(x)\n",
    "        if 'isa' in x and x.replace('isa', 'iza') in wlst:\n",
    "            wlst[x.replace('isa', 'iza')] += wlst[x]\n",
    "            wlst.pop(x)\n",
    "        if 'isi' in x and x.replace('isi', 'izi') in wlst:\n",
    "            wlst[x.replace('isi', 'izi')] += wlst[x]\n",
    "            wlst.pop(x)\n",
    "        if 'logue' in x and x.replace('logue', 'log') in wlst:\n",
    "            wlst[x.replace('logue', 'log')] += wlst[x]\n",
    "            wlst.pop(x)\n",
    "        if 'logu' in x and x.replace('logu', 'log') in wlst:\n",
    "            wlst[x.replace('logu', 'log')] += wlst[x]\n",
    "            wlst.pop(x)\n",
    "    return wlst\n",
    "\n",
    "def edge_scan(words, side, depth=0, thresholds=None, merge=True):\n",
    "    if side not in ('r', 'l'): raise ValueError('Invalid Side')\n",
    "    if not isinstance(depth, int) or depth < 2: raise ValueError(\"Invalid Depth\")\n",
    "    if thresholds:\n",
    "        if not isinstance(thresholds, Container) or any(not isinstance(y, int) for y in thresholds): raise ValueError(\"Invalid Thresholds\")\n",
    "    if not depth:\n",
    "        depth = int(np.average([len(x) for x in words]))\n",
    "        depth += (2 if depth > 4 else 1)\n",
    "    else: depth += 1\n",
    "\n",
    "    ecnt = Counter()\n",
    "    for w in words[1:]:\n",
    "        for i in range(2, depth):\n",
    "            tgt = (w[-i:] if side == 'r' else w[:i])\n",
    "            if ' ' in tgt: break\n",
    "            else: ecnt[tgt] += 1\n",
    "    for x in [x[0] for x in ecnt.most_common() if x[1] < (3 if thresholds else 2)]: ecnt.pop(x)\n",
    "\n",
    "    if merge:\n",
    "        fltr, vmerge = [], []\n",
    "        for x in [x for x in ecnt.most_common() if (x[0][0] if side == 'r' else x[0][-1]) not in ('a', 'e', 'i', 'o', 'u')]:\n",
    "            matches = [y for y in ecnt if x[0] in y and len(y) == len(x[0])+1 and (y[0] if side == 'r' else y[-1]) in ('a', 'e', 'i', 'o', 'u')]\n",
    "            if len(matches) > 1:\n",
    "                if sum([ecnt[y] for y in matches]) > (x[1]*0.85 if x[1] >= 50 else (x[1]-5 if x[1] > 12 else x[1]-3)):\n",
    "                    fltr.extend(matches)\n",
    "                    vmerge.append((x[0], x[1]))\n",
    "        for x in fltr: ecnt.pop(x)\n",
    "        \n",
    "        fltr = []\n",
    "        for x in ecnt.most_common():\n",
    "            matches = [y for y in ecnt if x[0] != y and x[0] in y and y not in fltr]\n",
    "            for y in matches:\n",
    "                if x[1] == ecnt[y]:\n",
    "                    fltr.append(y)\n",
    "                    break\n",
    "                if ecnt[y] > (x[1]*0.9 if x[1] >= 50 else (x[1]-3 if x[1] > 12 else x[1]-2)):\n",
    "                    fltr.append(y)\n",
    "\n",
    "    if thresholds and isinstance(thresholds, Container):\n",
    "        thresholds = {i+2: x for i, x in enumerate(thresholds)}\n",
    "        if len(thresholds) < depth:\n",
    "            for i in range(len(thresholds)+2, depth):\n",
    "                thresholds[i] = 3\n",
    "    elif thresholds == True:\n",
    "        cs = (min(max(2**(16/len(words)), 0), 2) - 1)**0.333\n",
    "        if cs < 0.10: thresholds = [int(max(min(950/(y**2.22), 768), 3)) for y in range(1, depth+1)]\n",
    "        elif cs > 0.60: thresholds = [int(max(min((x/4)/(y**2.22), 16), 3)) for y in range(1, depth+1)]\n",
    "        else:  thresholds = [int(max(min((0.75 if y == 1 else 1) * x*cs / (y**2.22), 768), 3)) for y in range(1, depth+1)]\n",
    "    else: thresholds = {i: 2 for i in range(2, depth+1)}\n",
    "\n",
    "    if merge:\n",
    "        for x in vmerge:\n",
    "            if x[0] in ecnt:\n",
    "                ecnt.pop(x[0])\n",
    "                ecnt[(f'_{x[0]}' if side == 'r' else f'{x[0]}_')] = x[1]\n",
    "        return [x for x in ecnt.most_common() if x[0] not in fltr and x[1] >= thresholds[len(x[0])]]\n",
    "    else: return [x for x in ecnt.most_common() if x[1] >= thresholds[len(x[0])]]\n",
    "\n",
    "\n",
    "\n",
    "class AffixAnalyzer:\n",
    "\n",
    "    def __init__(self, words, load_id: int=0):\n",
    "        self.ldct = {\n",
    "            'alpha': {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'},\n",
    "            '0afx': {'s_', 'd_', 'r_', 'n_', 't_', 'y_'},\n",
    "            '1afx': {'s_', 'd_', 'r_', 'n_', 't_', 'y_', '_a', 'a_', '_o', 'o_', 'i_', '_e'},\n",
    "            '2afx': {\n",
    "                    \"_de\", \"_di\", \"_bi\", \"_co\", \"_en\", \"_in\", \"_re\", \"_un\", \n",
    "                    \"_ab\", \"_ad\", \"_be\", \"_ec\", \"_em\", \"_ex\", \"_im\", \"_ob\", \n",
    "                    \"er_\", \"es_\", \"ed_\", \"ic_\", \"al_\", \"ry_\", \"ly_\", \n",
    "                    \"ar_\", \"cy_\", \"ee_\", \"en_\", \"ia_\", \"ie_\", \"or_\", \"um_\",\n",
    "                    \"_up\", \"up_\", \"_on\", \"_by\"\n",
    "            },\n",
    "            'fdbl': {'b', 'c', 'd', 'f', 'g', 'l', 'm', 'n', 'p', 'r', 's', 't'},\n",
    "            'bdbl': {'b', 'd', 'g', 'm', 'n', 'p', 'r', 't'},\n",
    "            'vwl1': {'a', 'e', 'i', 'o', 'u', 'y'},\n",
    "            'vwl2': {'a', 'e', 'i', 'o', 'u'},\n",
    "            'vwl3': {'a', 'e', 'o', 'i', 'y'},\n",
    "            'vwl4': {'a', 'e', 'o', 'u'},\n",
    "            'vwl5': {'a', 'o', 'i', 'u'},\n",
    "            'vwl6': {'i', 'u'},\n",
    "            'vwl7': {'e', 'o', 'a'},\n",
    "            'readdf': {'_ill', '_app', '_aff', '_irr', '_att', '_agg', '_opp', '_ass', '_all', '_ann', '_eff', '_acc'},\n",
    "            'bridges': {'emat', 'isat', 'izat', 'ibil', 'ula', 'at', 'an', 'ar', 'ti', 'a', 'e', 'i', 'o', 'u'}\n",
    "        }\n",
    "        self.verif = [\n",
    "            '_electr', '_econom', '_neuro', '_hydro', '_chrom', '_onto', '_onco', '_kine', '_lys', '_eco', '_bio', '_geo',\n",
    "            '_super', '_supra', '_under', '_trans', '_ortho', '_intra', '_inter', '_vert', '_over', '_fore', '_sup', '_sub', '_out', '_off', '_mid',\n",
    "            '_multi', '_micro', '_hyper', '_hypo', '_semi', '_poly', '_mono', '_uni', '_iso', '_lat', '_dia',\n",
    "            '_counter', '_pseudo', '_para', '_meta', '_auto', '_anti', '_pro', '_pre', '_non', '_mis', '_epi', '_sym', '_con', '_com', '_ant', '_ana', '_dis',\n",
    "            'cide_', 'ment_', 'logy_', 'cian_', 'less_', 'ness_', 'ance_', 'ence_', 'able_', 'ible_', 'ular_',\n",
    "            'tion_', 'sion_', 'ing_', 'ism_', 'ish_', 'ist_', 'ise_', 'ize_', 'ive_', 'ium_', 'ian_', 'ile_', \n",
    "            'ate_', 'ant_', 'ent_', 'est_', 'eum_', 'ean_', 'eur_', 'our_', 'ous_', 'oid_', 'sis_', 'ful_', \n",
    "            '_opp', '_irr', '_ill', '_eff', '_att', '_ass', '_app', '_all', '_agg', '_aff', '_acc', '_ad', \n",
    "            '_ab', '_an', '_ob', '_ec', '_en', '_ex', '_em', '_in', '_im',\n",
    "            'es_', 'er_', 'or_', 'ed_', 'ic_', 'al_', 'fy_', 'ty_', 'cy_', 'ly_', 'ry_', 'ia_', 'ie_', 'um_', 'en_', 'ar_', 'ee_',\n",
    "            '_up', '_on', '_by', '_be', 'up_', '_un', '_re', '_di', '_de', '_co', '_bi', '_e', 'd_', 'r_', 's_', 'y_'\n",
    "        ]\n",
    "        self.roots = [\n",
    "            '_lymph_', '_metre_', '_meter_', '_metry_', '_graph_', '_photo_', '_sume_', '_cede_', '_ceed_', '_ecto_', '_tone_', '_fish_', '_form_', \n",
    "            '_ship_', '_man_', '_men_', '_var_', '_max_', '_min_', '_lyr_', '_gress_', '_cess_', '_fess_', '_press_'\n",
    "        ]\n",
    "        self.cterms = {x for x in self.verif if len(x) > 3}\n",
    "        self.averif = {*self.ldct['2afx'], *self.ldct['1afx'], *self.verif}\n",
    "        self.cleared, self.failed_brk, self.final = Counter(), Counter(), Counter()\n",
    "        self.afxscore, self.wparts = dict(), []\n",
    "        self.dbg = False\n",
    "        if load_id:\n",
    "            self.load(load_id)\n",
    "            self.bare = {x.strip('_') for x in self.full_scores}\n",
    "            self.default_search = self.full_scores\n",
    "        else:\n",
    "            self.wlst = Counter({f'_{x[1]}_': int(x[0]) for x in words[::-1]})\n",
    "            self.full_scores = self.wlst.copy()\n",
    "            self.default_search = self.full_scores\n",
    "            self.bare = {x.strip('_') for x in self.full_scores}\n",
    "            self.pre_clean()\n",
    "            self.create_afx('w', 11, 2)\n",
    "            self.post_clean('w')\n",
    "            self.prep_entropy_calc()\n",
    "\n",
    "    def create_afx(self, method='w', vmax: int=0, vmin: int=0, rmax: int=7) -> None:\n",
    "        \"\"\"\n",
    "        Create list of affixes via 1 of 2 methods.\n",
    "        w: Window method moves a window of various sizes over all words and counts occurances of affixes\n",
    "        r: Remainder method for all words, find nested words and removes the inner word from all containing words and counts the remaining affixes\n",
    "\n",
    "        w ex: '_retracting_' -> ret, etr, tra, rac, act, cti, tin, ing\n",
    "        r ex: 'firm' | 'reaffirmed', 'confirming' -> reaf, ed, con, ing\n",
    "\n",
    "        Args:\n",
    "            vmax (int, optional): Maximum window size for w method || Maximum inner word length for r method. Defaults to 10 for w | 12 for r.\n",
    "            vmin (int, optional): Minimum window size for w method || Minimum inner word length for r method. Defaults to 2 for w | 4 for r.\n",
    "            rmin (int, optional): Minimum word length for outer words. r method only. Defaults to 7.\n",
    "        \"\"\"\n",
    "        self.afx = Counter()\n",
    "        if method == 'w':\n",
    "            if not vmax: vmax = 10\n",
    "            if not vmin: vmin = 2\n",
    "\n",
    "            for word in self.wlst:\n",
    "                wln = min(len(word), vmax)\n",
    "                for n in range(vmin, wln-1):\n",
    "                    for pos in range(wln-n+1):\n",
    "                        self.afx[word[pos:pos+n]] += self.wlst[word]\n",
    "\n",
    "        elif method == 'r':\n",
    "            if not vmax: vmax = 12\n",
    "            if not vmin: vmin = 4\n",
    "\n",
    "            smalls = {x for x in self.wlst if len(x) > vmin and len(x) < vmax}\n",
    "            bigs = {x for x in self.wlst if len(x) > rmax}\n",
    "            for x in tqdm(smalls):\n",
    "                x = x.strip('_')\n",
    "                group = [y for y in bigs if x in y]\n",
    "                for y in group:\n",
    "                    out = y.split(x)\n",
    "                    if len(out) > 2:\n",
    "                        out.append(f'_{out[1]}')\n",
    "                        out.append(f'{out[1]}_')\n",
    "                        out.pop(1)\n",
    "                    for z in out:\n",
    "                        if z not in ('_', ''):\n",
    "                            self.afx[z] += 1\n",
    "        print('Frags Created', len(self.afx))\n",
    "\n",
    "    def pre_clean(self) -> None:\n",
    "        #Remove ' words from word list\n",
    "        for x in [\"_ain't_\", \"_can't_\", \"_won't_\", \"_shan't_\"]:\n",
    "            self.final[\"n't_\"] += self.wlst[x]\n",
    "        for x in [\"_i'm_\", \"_can't_\"]:\n",
    "            self.wlst[f'{x[:-3]}_'] += self.wlst[x]\n",
    "        for x in [\"_ma'am_\", \"_ain't_\", \"_i'm_\"]:\n",
    "            self.final[x] += self.wlst[x]\n",
    "        for x in [\"_ain't_\", \"_can't_\", \"_won't_\", \"_shan't_\", \"_ma'am_\", \"_i'm_\", \"_van't_\"]:\n",
    "            self.wlst.pop(x)\n",
    "        for x in [z for z in self.wlst if \"'\" in z if any(y in z for y in [\"'s_\", \"'ll_\", \"'ve_\", \"n't_\", \"'re_\", \"'d_\"])]:\n",
    "            for efx in [\"'s_\", \"'ll_\", \"'ve_\", \"n't_\", \"'re_\", \"'d_\"]:\n",
    "                if efx in x:\n",
    "                    self.final[efx] += self.wlst[x]\n",
    "                    self.wlst[x.replace(efx, '_')] += self.wlst[x]\n",
    "                    self.wlst.pop(x)\n",
    "        for x in [z for z in self.wlst if \"'\" in z]:\n",
    "            self.wlst.pop(x)\n",
    "\n",
    "    def post_clean(self, regime: str, cdist: int=0, cmin: int=0) -> None:\n",
    "        \"\"\"\n",
    "        Clean affix list, remove noise and words that will be tokenized as wholes\n",
    "        \n",
    "        Args:\n",
    "            cdist (int, optional): Distance to search from a sorted affix list for nested affix cleaning. Defaults to 2048 for w | 512 for r.\n",
    "            cmin (int, optional): Minimum occurrences to keep an affix in the list. Defaults to 64 for w | 8 for r.\n",
    "        \"\"\"\n",
    "        if regime == 'w':\n",
    "            if not cdist: cdist = 2048\n",
    "            if not cmin: cmin = 32\n",
    "        elif regime == 'c':\n",
    "            if not cdist: cdist = 512\n",
    "            if not cmin: cmin = 8\n",
    "\n",
    "        for x in self.wlst.most_common():\n",
    "            if x[1] > 3000000: self.cleared[x[0]] = x[1] #Words occurance > 3M\n",
    "            elif x[1] > 100000 and len(x[0]) < 6: self.cleared[x[0]] = x[1] #Words, < 4 chars, occurance > 100k\n",
    "            elif x[1] < 100000: break\n",
    "        for z in [y for y in self.wlst if len(y) < 5]: #Words 2 letters or less\n",
    "            if self.wlst[z] > cmin: self.cleared[z] += self.wlst[z] \n",
    "            else: self.wlst.pop(z)\n",
    "        for z in self.cleared: #Remove cleared words\n",
    "            if z in self.wlst: self.wlst.pop(z)\n",
    "\n",
    "        #Scan from most common to least, if a nested affix is found within 2048 items, subtract that items value from the current affix\n",
    "        afidx = [x[0] for x in self.afx.most_common() if '_' in x[0]]\n",
    "        for i, x in enumerate(afidx):\n",
    "            group = [y for y in afidx[i+1:i+1+cdist] if x in y]\n",
    "            if group: self.afx[x] -= self.afx[group[0]]\n",
    "        self.afx = Counter({x[0]: x[1] for x in self.afx.most_common() if x[1] > cmin})\n",
    "\n",
    "        for z in [x for x in self.afx if '_' not in x and len(x) < 3]:\n",
    "            self.afx.pop(z) #Unattached affixes < 3 chars\n",
    "        for z in [x for x in self.afx if not any(y in x for y in self.ldct['vwl1']) and x not in self.ldct['1afx'] and x not in self.ldct['2afx']]:\n",
    "            self.afx.pop(z) #Affixes with no vowels\n",
    "        for z in [x for x in self.afx if len(x) < 4 and '_' in x and x not in self.averif]:\n",
    "            self.afx.pop(z) #Affixes < 3 chars that arent in pre verified list\n",
    "        for z in {x[0] for x in [(x, f'_{x}', f'{x}_', f'_{x}_') for x in self.afx if '_' not in x] if (x[1] in self.afx or x[2] in self.afx or x[3] in self.cleared or x[3] in self.final or x[3] in self.wlst)}:\n",
    "            self.afx.pop(z) #Unattached affixes with that have an attached variant\n",
    "        for z in tqdm([x for x in self.afx]):\n",
    "            c = 0\n",
    "            for y in self.wlst:\n",
    "                if z in y: c += 1\n",
    "                if c > 3: break\n",
    "            else:  self.afx.pop(z)\n",
    "\n",
    "        for x in ('less_', 'ness_'): self.target_removal(x, exe=True)\n",
    "        self.target_removal('es_', exc1=('es_', 's_'), exc2=('is_', 'us_', 'ss_', 'series_', 'species_'), exe=True)\n",
    "        self.target_removal('s_', exc1=('es_', 's_'), exc2=('is_', 'us_', 'ss_', 'series_', 'species_'), exe=True)\n",
    "        self.target_removal('er_', exc2=('meter_', 'over_', 'under_', 'master_'), exe=True)\n",
    "        self.target_removal('or_', exc2=('oor_'), exe=True)\n",
    "        self.target_removal('ed_', exc2=('eed_'), exe=True)\n",
    "        for x in ('en_', 'ly_', 'ion_', 'ous', 'ing_', 'ity_', 'ize_', 'ise_', 'ive_', 'ist_', 'ism_', 'ory_', 'est_', 'ment_', 'ant_', 'ary_', 'ate_', 'ic_', 'al_'): self.target_removal(x, exe=True)\n",
    "        self.target_removal('y_', exc2=('ity_', 'ry_', 'ly_', 'ory_', 'ary_'), exe=True)\n",
    "\n",
    "    def _prep_afx(self) -> None:\n",
    "        with open(r'D:\\dstore\\nlp\\w2v\\directions', 'rb') as f:\n",
    "            directions = pickle.load(f)\n",
    "        self.afx['_a'] += self.wlst[\"_around_\"]\n",
    "        self.afx[\"_round\"] += self.wlst[\"_around_\"]\n",
    "        self.afx[\"_o\"] += self.wlst[\"_over_\"]\n",
    "        self.afx[\"_ver\"] += self.wlst[\"_over_\"]\n",
    "        for x in directions[0]:\n",
    "            self.afx[x[:-1]] += self.wlst[x]\n",
    "        for x in directions[2]:\n",
    "            self.afx[x[:-1]] += self.wlst[x]\n",
    "            self.afx[x[1:]] += self.wlst[x]\n",
    "        for x in directions[1]:\n",
    "            self.afx[x[1:]] += self.wlst[x]\n",
    "        for y in directions:\n",
    "            for x in y:\n",
    "                if len(x) < 5:\n",
    "                    self.final[x] += self.wlst[x]\n",
    "                    self.wlst.pop(x)\n",
    "                else:\n",
    "                    self.cleared[x] += self.wlst[x]\n",
    "                    self.wlst.pop(x)\n",
    "\n",
    "    def prep_entropy_calc(self, over_length: int=7, pull_cutoff: int=2, wgts: list[list[int]]=[[1, 1, 1.25], [1, 1.25, 1.5], [1.25, 1.5, 1.75]]) -> None:\n",
    "        \"\"\"\n",
    "        Goal: \n",
    "            To filter out fragments of affixes from whole affixes\n",
    "        Hypothesis: \n",
    "            The distribution of letters adjacent to an affix will help me determine whether or not an affix is whole or not.\n",
    "            Partial affixes will have much lower entropy in atleast one of the measurements because the letter that completes the affix will dominate the distribution.\n",
    "            Limiting the sampling window size will give amplify the entropies.\n",
    "        Example: \n",
    "            ng_ is a partial affix of ing_.\n",
    "            When sampling letters to the left of ng_, the letter 'i's dominate the distribution.\n",
    "            When the window size of is set to 1, the distribution of letters will be almost entirely 'i' giving a very high relative entropy value.\n",
    "            In contrast with the distribution of ing_ the distribution will be much closer to the general distribution of the whole data set\n",
    "        Args:\n",
    "            over_length (int, optional): Maximum length for standard affixes. Affixes longer than this value will have their character distributions separated. Should be slightly over half the length of the average word. Defaults to 7.\n",
    "            pull_cutoff (int, optional): Maximum difference in affix length when searching for parent / child affix nodes. Defaults to 2.\n",
    "            wgts (list[list[int]], optional): 3x3 Weight matrix for scaling direction and window size of relative entropy calculations. Defaults to [[1, 1, 1.25], [1, 1.25, 1.5], [1.25, 1.5, 1.75]].\n",
    "        Returns:\n",
    "            re_arr, dsts, rntp, drntp dictionaries added to instance\n",
    "        \"\"\"\n",
    "        dsts, pd, sd, nd = {}, Counter(), Counter(), Counter()\n",
    "        for x in self.wlst: # Get letter distributions for: letters in front half of words, letters in back half of words, all letters\n",
    "            x = x.strip('_')\n",
    "            for l in x: nd[l] += 1\n",
    "            i = round((len(x)+0.1) / 2)\n",
    "            for l in x[:i]: pd[l] += 1\n",
    "            for l in x[-i:]: sd[l] += 1\n",
    "        pd = Counter({x[0]: x[1] / pd.total() for x in pd.most_common()})\n",
    "        sd = Counter({x[0]: x[1] / sd.total() for x in sd.most_common()})\n",
    "        nd = Counter({x[0]: x[1] / nd.total() for x in nd.most_common()})\n",
    "\n",
    "        rntp = {} # Calculate the relative entropy of letters adjacent to an affix\n",
    "        for x in tqdm(self.afx): \n",
    "            hold = []\n",
    "            if '_' not in x:\n",
    "                fd = nd\n",
    "                for i in range(1, 4): #Define sampling window size\n",
    "                    o1, o2 = self.surrounds(x, i)\n",
    "                    hold.append([self.kld(self.surrounds(x, i, merge=True), fd), self.kld(o1, fd), self.kld(o2, fd)])\n",
    "            else:\n",
    "                if x[0] == '_': pre = True\n",
    "                else: pre = False\n",
    "                if (pre and len(x) > over_length) or (not pre and len(x) <= over_length): fd = sd\n",
    "                else: fd = pd\n",
    "                for i in range(1, 4):\n",
    "                    o1, o2 = self.surrounds(x, i) # Define direction of window here\n",
    "                    if pre: side = o2 \n",
    "                    else: side = o1\n",
    "                    hold.append([self.kld(self.surrounds(x, i, merge=True), fd), self.kld(side, fd), self.kld(self.surrounds(x, i, exact=True), fd)])\n",
    "            rntp[x] = np.array(hold[::-1]).T\n",
    "\n",
    "        re_arr, hold = {}, [] # Get letter distributions for the first and last: 1, 2, 3 letters\n",
    "        for i in range(1, 4):\n",
    "            frel, brel = Counter(), Counter()\n",
    "            for x in self.wlst:\n",
    "                if len(x) >= 3+i:\n",
    "                    x = x.strip('_')\n",
    "                    for l in x[-i:]: brel[l] += 1\n",
    "                    for l in x[:i]: frel[l] += 1\n",
    "            hold.append([self.kld(frel, pd), self.kld(brel, sd)])\n",
    "            dsts[f'pd{i}'] = Counter({x[0]: x[1] / frel.total() for x in frel.most_common()})\n",
    "            dsts[f'sd{i}'] = Counter({x[0]: x[1] / brel.total() for x in brel.most_common()})\n",
    "        hold = [[x]*3 for x in np.array(hold[::-1]).T]\n",
    "        #These will be used for calculating the derivatives of root/leaf affixes\n",
    "        re_arr['pd3'], re_arr['sd3'] = np.array(hold[0]), np.array(hold[1])\n",
    "        re_arr['lpd3'], re_arr['lsd3'] = np.array([rntp[x] for x in self.afx if len(x) > over_length and x[0] == '_']).mean(axis=0), np.array([rntp[x] for x in self.afx if len(x) > over_length and x[-1] == '_']).mean(axis=0)\n",
    "\n",
    "        drntp = {}\n",
    "        for x in tqdm(self.afx): # Derivative of relative entropy values along a sequential chain of affixes\n",
    "            if '_' in x: #Chains can only beformed with positional affixes\n",
    "                above, below = self.pulld(x, pull_cutoff), self.pullu(x)\n",
    "                if above: above = np.array([rntp[y] for y in above]).mean(axis=0)\n",
    "                else: #If an affix has no affixes above it, use the averaged relative entropies for affixes longer than 6 letters to calculate\n",
    "                    above = self.pulld(x)\n",
    "                    if above: above = np.array([rntp[y] for y in above]).mean(axis=0)\n",
    "                    elif x[0] == '_': above = re_arr['lpd3']\n",
    "                    elif x[-1] == '_': above = re_arr['lsd3']\n",
    "                if below: below = rntp[below]\n",
    "                elif x[0] == '_': below = re_arr['pd3']\n",
    "                else: below = re_arr['sd3']\n",
    "                middle = rntp[x]\n",
    "                drntp[x] = (above-middle) - (middle-below)\n",
    "\n",
    "        #Find the average derivatives for edge cases\n",
    "        wgts = np.array(wgts)\n",
    "        re_arr['lpd3x'], re_arr['lsd3x'] = re_arr['lpd3']*wgts, re_arr['lsd3']*wgts\n",
    "        re_arr['pd3x'], re_arr['sd3x'] = re_arr['pd3']*wgts, re_arr['sd3']*wgts\n",
    "        re_arr['dlpd3'], re_arr['dlsd3'] = np.mean([(rntp[x] * wgts) - re_arr['lpd3x'] for x in self.afx if x[0] == '_' and len(x) > over_length], axis=0), np.mean([(rntp[x] * wgts) - re_arr['lsd3x'] for x in self.afx if x[-1] == '_' and len(x) > over_length], axis=0)\n",
    "        re_arr['dpd3'], re_arr['dsd3'] = np.mean([(rntp[x] * wgts) - re_arr['pd3x'] for x in self.afx if x[0] == '_' and len(x) <= over_length], axis=0), np.mean([(rntp[x] * wgts) - re_arr['sd3x'] for x in self.afx if x[-1] == '_' and len(x) <= over_length], axis=0)\n",
    "        dsts['pd'], dsts['sd'], dsts['nd'] = pd, sd, nd\n",
    "        re_arr['wgts'], re_arr['null'] = wgts, [np.array([[0]*3]*3), np.array([[0]*3]*3)]\n",
    "        self.re_arr, self.dsts, self.rntp, self.drntp = re_arr, dsts, rntp, drntp\n",
    "\n",
    "    def get_compounds(self):\n",
    "        combos = {}\n",
    "        for x in self.wlst:\n",
    "            front, back = [], []\n",
    "            for y in self.wlst:\n",
    "                if y != x:\n",
    "                    f, b = y[1:], y[:-1]\n",
    "                    if f in x and (len(x) - x.index(f) - len(f)) == 0:\n",
    "                        front.append(f)\n",
    "                    if b in x and x.index(b) == 0:\n",
    "                        back.append(b)\n",
    "            veri = []\n",
    "            if front and back:\n",
    "                for m in front:\n",
    "                    for n in back:\n",
    "                        if len(m) + len(n) == len(x):\n",
    "                            veri.append((m, n))\n",
    "            if veri:\n",
    "                combos[x] = tuple(veri)\n",
    "        return combos\n",
    "\n",
    "    def search(self, term: str, corpus: Container=None, exc: str|tuple[str]=None, pos: bool=False, sfil=False, svar='i'):\n",
    "        #Returns all items that contain the input affix\n",
    "        if not corpus: corpus = self.default_search\n",
    "        if svar == 's': res = [x.strip() for x in lrsort([x for x in corpus if x.startswith(term)])]\n",
    "        elif svar == 'e': res = [x.strip() for x in rrsort([x for x in corpus if x.endswith(term)])]\n",
    "        else: res = sorted([x for x in corpus if term in x])\n",
    "\n",
    "        if exc: res = [x for x in res if all(y not in x for y in ((exc,) if isinstance(exc, str) else exc))]\n",
    "        if sfil: res = [x for x in res if x not in {f'_{term.strip(\"_\")}', f'{term.strip(\"_\")}_', f'_{term.strip(\"_\")}_', term}]\n",
    "        if pos: res = [x for x in res if '_' in x]\n",
    "        return res\n",
    "    \n",
    "    def is_sub(self, orig, rep):\n",
    "        orig, rep = orig.strip('_'), rep.strip('_')\n",
    "        rslt = [x.split(rep) for x in self.search(rep) if x != f'_{rep}_']\n",
    "        orslt = [x.split(orig) for x in self.search(orig) if x != f'_{orig}_']\n",
    "        if len(rslt) > 3 and len(orslt) > 3:\n",
    "            if len([x for x in orslt if x in rslt]) > 1: return False\n",
    "        out = {y for x in rslt for y in x if y != '_' and y in self.verif}\n",
    "        if (rslt and out) and (len(out) > 7 or len(out) / len(rslt) >= 0.5 or (len(out) / len(rslt) >= 0.15 and len(out) > 2)): return True\n",
    "        else: return False\n",
    "\n",
    "    def gsub(self, target: str, afx: str, amode: int=0, best: bool=True, fltr: bool=True) -> str:\n",
    "        \"\"\"\n",
    "        Remove the affix from a word following english rules, returning the proper root\n",
    "        Will not work if the remaining word/affix is too short (2 chars for word, 1 char for affix)\n",
    "\n",
    "        Args:\n",
    "            target (str): Target word to remove affix from\n",
    "            afx (str): Affix to remove from target word\n",
    "            best (bool, optional): Whether to return the best option or all options. Defaults to True.\n",
    "            amode (int, optional): Verification list. 0 verifies against words list. 1 verifies against affix list. 2 verifies against both with affix and co-affix. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            (str): Target with affix removed in its neutral form\n",
    "        \"\"\"\n",
    "        if not amode and len(target) - len(afx) < 3: return\n",
    "        elif amode and len(target) - len(afx) < 2: return\n",
    "        spafx = {'sion_', 'ian_', 'es_', 'cy_', 's_', 'y_'}\n",
    "        rep = target.replace(afx, '')\n",
    "        candidates = [target.replace(afx, '_')]\n",
    "        if afx[0] == '_': pre = True\n",
    "        else: pre = False\n",
    "\n",
    "        if afx in spafx: #Specific Affix Substitution Rules\n",
    "            if afx == 'sion_':\n",
    "                if rep.endswith('is'): candidates.append(f'{rep[:-1]}t_')\n",
    "                elif rep[-1] == 'n': candidates.append(f'{rep[:-1]}d_')\n",
    "                elif rep[-1] == 'r': candidates.append(f'{rep}t_')\n",
    "                elif rep[-1] in self.ldct['vwl2']:\n",
    "                    candidates.append(f'{rep}de_')\n",
    "                    candidates.append(f'{rep}re_')\n",
    "            elif afx == 'tion_':\n",
    "                if rep.endswith('lu'):\n",
    "                    candidates.append(f'{rep[:-1]}ve_')\n",
    "            elif afx == 'ian_':\n",
    "                if rep.endswith('ar'):\n",
    "                    candidates.append(f'{rep[:-2]}_')\n",
    "            elif afx == 'cy_':\n",
    "                candidates.append(f'{rep}t_')\n",
    "                candidates.append(f'{rep}ce_')\n",
    "                if rep[-1] == 'a': candidates.append(f'{rep}te_')\n",
    "            elif afx == 'es_':\n",
    "                if rep[-1] == 'v':\n",
    "                    candidates.append(f'{rep[:-1]}f_')\n",
    "                    candidates.append(f'{rep[:-1]}fe_')\n",
    "                elif rep.endswith('ic'):\n",
    "                    candidates.append(f'{rep[:-2]}ex_')\n",
    "            elif afx == 's_':\n",
    "                if rep[-1] in ['s', 'i', 'u']: return\n",
    "            elif afx == 'y_':\n",
    "                if rep[-1] in self.ldct['vwl2']: return\n",
    "\n",
    "        if pre:\n",
    "            if len(afx) == 2:\n",
    "                if f'_{rep}' in self.full_scores: return f'_{rep}'\n",
    "                else: return\n",
    "\n",
    "            if afx[-1] not in self.ldct['vwl2']: candidates.append(f'_{afx[-1]}{rep}')\n",
    "            #if rep[0] in self.ldct['vwl2']: candidates.append(f'_{rep[1:]}')\n",
    "            else:\n",
    "                if len(rep) > 4 and rep[0] == rep[1] and rep[0] in self.ldct['fdbl']:\n",
    "                    candidates.append(f'_{rep[1:]}')\n",
    "\n",
    "        else:\n",
    "            if len(afx) == 2:\n",
    "                if f'{rep}_' in self.full_scores: return f'{rep}_'\n",
    "            elif len(afx) > 2: candidates.append(f'{rep}e_')\n",
    "\n",
    "            if afx[0] not in self.ldct['vwl2']:\n",
    "                candidates.append(f'{rep}{afx[0]}_')\n",
    "                candidates.append(f'{rep}{afx[0]}e_')\n",
    "                #v cfx_\n",
    "                if rep[-1] in self.ldct['vwl2']:\n",
    "                    candidates.append(f'{rep[:-1]}_')\n",
    "                    candidates.append(f'{rep[:-1]}e_')\n",
    "                    if rep[-1] == 'i': candidates.append(f'{rep[:-1]}y_')\n",
    "                #c cfx_\n",
    "                else: pass\n",
    "            else:\n",
    "                #v vfx_\n",
    "                if rep[-1] in self.ldct['vwl2']:\n",
    "                    candidates.append(f'{rep[:-1]}_')\n",
    "                    candidates.append(f'{rep[:-1]}e_')\n",
    "                    if rep[-1] == 'i': candidates.append(f'{rep[:-1]}y_')\n",
    "                #c vfx_\n",
    "                else:\n",
    "                    if len(rep) > 4 and rep[-1] == rep[-2] and rep[-1] in self.ldct['bdbl']:\n",
    "                        candidates.append(f'{rep[:-1]}_')\n",
    "\n",
    "        if target in candidates: candidates.remove(target)\n",
    "        if candidates and fltr:\n",
    "            candidates = set(candidates)\n",
    "            if amode == 0: out = sorted([(x, 50000) if x in self.roots else (x, self.full_scores[x]) for x in candidates if x in self.full_scores], key=lambda x: np.log(x[1] * (len(x[0])-1)))\n",
    "            elif amode == 1: out = sorted([(x, self.afx[x]) for x in candidates if x in self.afx], key=lambda x: np.log(x[1] * (len(x[0])-1)))\n",
    "            elif amode == 2:\n",
    "                out = []\n",
    "                if pre:\n",
    "                    for x in candidates:\n",
    "                        mafx, full = f'{x[1:]}_', f'{x}_'\n",
    "                        if mafx in self.afx and self.afx[mafx] > 8: out.append((mafx, self.afx[mafx]))\n",
    "                        elif full in self.wlst and self.wlst[full] > 256: out.append((full, np.log2(self.wlst[full])))\n",
    "                else:\n",
    "                    for x in candidates:\n",
    "                        mafx, full = f'_{x[:-1]}', f'_{x}'\n",
    "                        if mafx in self.afx and self.afx[mafx] > 8: out.append((mafx, self.afx[mafx]))\n",
    "                        elif full in self.wlst and self.wlst[full] > 256: out.append((full, np.log2(self.wlst[full])))\n",
    "                out = sorted(out, key=lambda x: np.log(x[1] * (len(x[0])-1)))\n",
    "            if out:\n",
    "                if best: return out[-1][0]\n",
    "                else: return out\n",
    "        else: return candidates\n",
    "\n",
    "    def target_removal(self, afx: str, exc1: str|tuple[str]=None, exc2: str|tuple[str]=None, exe: bool=False) -> list[str] | None:\n",
    "        \"\"\"\n",
    "        Find affixes that contain the input affix and runs the gsub method on them. If the 'exe' paremeter is set to true, found affixes will be removed.\n",
    "\n",
    "        Args:\n",
    "            afx (str): Affix to search/remove.\n",
    "            exc1 (str | tuple[str], optional): Exact affixes to exclude from removal. Defaults to None.\n",
    "            exc2 (str | tuple[str], optional): Affixes to filter for affixes to exclude from removal. Defaults to None.\n",
    "            exe (bool, optional): Remove found affixes. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list (str): List of substitutions for target affix\n",
    "        \"\"\"\n",
    "        if exc1 and exc2:\n",
    "            if isinstance(exc1, str) and isinstance(exc2, str): targets = [x for x in self.afx if afx in x and x not in (afx, exc1) and exc2 not in x]\n",
    "            elif isinstance(exc1, str): targets = [x for x in self.afx if afx in x and x not in (afx, exc1) and all(y not in x for y in exc2)]\n",
    "            elif isinstance(exc2, str): targets = [x for x in self.afx if afx in x and x not in (afx, *exc1) and exc2 not in x]\n",
    "            else: targets = [x for x in self.afx if afx in x and x not in (afx, *exc1) and all(y not in x for y in exc2)]\n",
    "        elif exc1:\n",
    "            if isinstance(exc1, str): targets = [x for x in self.afx if afx in x and x not in (afx, exc1)]\n",
    "            else: targets = [x for x in self.afx if afx in x and x not in (afx, *exc1)]\n",
    "        elif exc2:\n",
    "            if isinstance(exc2, str): targets = [x for x in self.afx if afx in x and x != afx and exc2 not in x]\n",
    "            else: targets = [x for x in self.afx if afx in x and x != afx and all(y not in x for y in exc2)]\n",
    "        else: targets = [x for x in self.afx if afx in x and x != afx]\n",
    "\n",
    "        if exe:\n",
    "            for x in targets:\n",
    "                tmp = self.gsub(x, afx, amode=1)\n",
    "                if tmp: \n",
    "                    self.afx[tmp] += self.afx[x]\n",
    "                    self.afx.pop(x)\n",
    "        else: return [res for x in targets if (res := self.gsub(x, afx, amode=1))]\n",
    "\n",
    "    def pulld(self, afx: str, len_lim: int=0) -> list[str]:\n",
    "        \"\"\"\n",
    "        Finds all child nodes of the input affix\n",
    "\n",
    "        Args:\n",
    "            afx (str): Input affix\n",
    "            len_lim (int, optional): Maximum difference in length between input and output nodes. 0 permits any difference. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of all child nodes\n",
    "        \"\"\"\n",
    "        sub_set = [x for x in self.afx if afx in x and x != afx]\n",
    "        if '_' not in afx: sub_set = [x for x in sub_set if '_' not in x]\n",
    "        out, aln = set(), len(afx)\n",
    "        for x in sub_set:\n",
    "            i = 1\n",
    "            if '_' not in x:\n",
    "                ti, wl = x.index(afx), len(x)\n",
    "                pf = wl-(ti+aln)\n",
    "                while i <= max(pf, ti):\n",
    "                    if x[ti-i:ti+aln] in sub_set and x != x[ti-i:ti+aln]: break\n",
    "                    elif x[ti:ti+aln+i] in sub_set and x != x[ti:ti+aln+i]: break\n",
    "                    i += 1\n",
    "                else: out.add(x)\n",
    "            elif x[0] == '_':\n",
    "                while len(x[:-i]) > aln:\n",
    "                    if x[:-i] in sub_set: break\n",
    "                    i += 1\n",
    "                else: out.add(x)\n",
    "            else:\n",
    "                while len(x[i:]) > aln:\n",
    "                    if x[i:] in sub_set: break\n",
    "                    i += 1\n",
    "                else: out.add(x)\n",
    "        if not len_lim: return out\n",
    "        else: return [x for x in out if len(x) <= aln+len_lim]\n",
    "\n",
    "    def pullu(self, afx: str) -> str:\n",
    "        #Return the parent node of the input affix\n",
    "        i = 1\n",
    "        if '_' not in afx:\n",
    "            while i < len(afx):\n",
    "                hold = []\n",
    "                if afx[:-i] in self.afx: hold.append(afx[:-1])\n",
    "                elif afx[i:] in self.afx: hold.append(afx[i:])\n",
    "                if len(hold) > 1: return hold\n",
    "                elif hold: return hold[0]\n",
    "                i += 1\n",
    "        elif afx[0] == '_':\n",
    "            while i < len(afx):\n",
    "                if afx[:-i] in self.afx: return afx[:-i]\n",
    "                i += 1\n",
    "        else:\n",
    "            while i < len(afx):\n",
    "                if afx[i:] in self.afx: return afx[i:]\n",
    "                i += 1\n",
    "\n",
    "    def chain(self, afx: str) -> list[str]:\n",
    "        #Return all nodes along the longest possible path that contains this affix node\n",
    "        out = sorted([x for x in self.afx if afx in x or x in afx], key=lambda x: len(x))\n",
    "        if out:\n",
    "            out = out[-1]\n",
    "            if '_' in afx: return sorted([x for x in self.afx if x in out and '_' in x], key=lambda x: len(x), reverse=True)\n",
    "            else: return sorted([x for x in self.afx if x in out and '_' not in x], key=lambda x: len(x), reverse=True)\n",
    "\n",
    "    def surrounds(self, afx: str, window: int=3, merge: bool=False, exact: bool=False) -> tuple[Counter] | Counter:\n",
    "        \"\"\"\n",
    "        Counts the letters adjacent to the input affix in all words from the word list.\n",
    "        By default the input affix will have its positional indicator _ removed.\n",
    "\n",
    "        Args:\n",
    "            afx (str): Target affix.\n",
    "            window (int, optional): Distance from input affix to count. Defaults to 3.\n",
    "            merge (bool, optional): Combine left and right side counts before returning. Defaults to False.\n",
    "            exact (bool, optional): Counted words must respect affixes positional indicator. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple[Counter] | Counter: Counts of letters adjacent to input affix.\n",
    "        \"\"\"\n",
    "        left_cnt, right_cnt = Counter(), Counter()\n",
    "        if not exact:\n",
    "            afx = afx.strip('_')\n",
    "            targets = [x.strip('_').split(afx) for x in self.wlst if afx in x]\n",
    "        else:\n",
    "            targets = [x.split(afx) for x in self.wlst if afx in x]\n",
    "            targets = [(x[0].strip('_'), x[1].strip('_')) for x in targets]\n",
    "\n",
    "        if not exact or (exact and afx[-1] == '_'):\n",
    "            for x in targets:\n",
    "                idx = min(len(x[0]), window)\n",
    "                if idx: left_cnt[x[0][-idx:]] += 1\n",
    "        if not exact or (exact and afx[0] == '_'):\n",
    "            for x in targets:\n",
    "                idx = min(len(x[1]), window)\n",
    "                if idx: right_cnt[x[1][:idx]] += 1\n",
    "        if merge or exact:\n",
    "            for x in left_cnt: right_cnt[x] += left_cnt[x]\n",
    "            return right_cnt\n",
    "        else: return Counter({x[0]: x[1] for x in left_cnt.items() if len(x[0]) == window}), Counter({x[0]: x[1] for x in right_cnt.items() if len(x[0]) == window})\n",
    "\n",
    "    def remean(self, rearr: np.ndarray) -> np.ndarray:\n",
    "        #Returns the mean array of the input array for each column and row\n",
    "        return np.array([*[np.mean(y) for y in rearr], *[np.mean(y) for y in rearr.T]])\n",
    "\n",
    "    def kld(self, P: Counter, Q: Counter=None, pfloor: int=0) -> float:\n",
    "        \"\"\"\n",
    "        Kullback Leibler Divergence Calculation\n",
    "\n",
    "        Args:\n",
    "            P (Counter): Counts of letters\n",
    "            Q (Counter, optional): Letter counts or distribution to compare against P. Defaults to letter distribution of entire the word list.\n",
    "            base_value (int, optional): Base count of all letters. Higher values reduces effect of 0s. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            float: Relative Entropy of the two counts / distributions.\n",
    "        \"\"\"\n",
    "        if not pfloor:\n",
    "            if P.total() < 156: pfloor = 1\n",
    "            elif P.total() < 312: pfloor = 2\n",
    "            else: pfloor = 3\n",
    "        if not Q: Q = self.dsts['nd']\n",
    "\n",
    "        pcnt = Counter({x: pfloor for x in self.ldct['alpha']})\n",
    "        for x in P:\n",
    "            for y in x: pcnt[y] += P[x]  \n",
    "        psum = sum(x for x in pcnt.values())\n",
    "        if Q.total() > 1.5:\n",
    "            qcnt = Counter({x: pfloor for x in self.ldct['alpha']})\n",
    "            for x in Q:\n",
    "                for y in x: qcnt[y] += Q[x]\n",
    "            for x in self.ldct['alpha']:\n",
    "                if pcnt[x] == pfloor and qcnt[x] == pfloor:\n",
    "                    pcnt.pop(x)\n",
    "                    qcnt.pop(x)\n",
    "            qsum = sum(x for x in qcnt.values())\n",
    "            return sum([(pcnt[x] / psum) * np.log2((pcnt[x] / psum) / (qcnt[x] / qsum)) for x in pcnt])\n",
    "        else: return sum([(pcnt[x] / psum) * np.log2((pcnt[x] / psum) / Q[x]) for x in pcnt])\n",
    "\n",
    "    def relent_peaks(self, afx: str, bridge_coeff: float=1.0, over_length: int=7) -> list[str, float]:\n",
    "        \"\"\"\n",
    "        Find affixes with significant variations from their parent/child nodes relative entropy.\n",
    "        Affix chain will be the longest chain that contains the input affix.\n",
    "        Specific chains can be targetted by inputting the longest affix of a chain.\n",
    "        Affixes returned indicate a target of interest for removal.\n",
    "\n",
    "        Args:\n",
    "            afx (str): Target affix\n",
    "            bridge_coeff (int, optional): Change in relative entropy to be considered significant. Defaults to 1.0.\n",
    "            over_length (int, optional): Maximum length for standard affixes. Affixes longer than this value will have their character distributions separated. Should be slightly over half the length of the average word. Defaults to 7.\n",
    "\n",
    "        Returns:\n",
    "            list[str, float]: List of affixes with significant relative entropy spikes\n",
    "        \"\"\"\n",
    "        if afx[0] == '_':\n",
    "            if len(afx) > over_length: scores = [self.re_arr['lpd3'].copy(), self.re_arr['lpd3'].copy()]\n",
    "            else: scores = [self.re_arr['pd3'].copy(), self.re_arr['pd3'].copy()]\n",
    "        elif afx[-1] == '_':\n",
    "            if len(afx) > over_length: scores = [self.re_arr['lsd3'].copy(), self.re_arr['lsd3'].copy()]\n",
    "            else: scores = [self.re_arr['sd3'].copy(), self.re_arr['sd3'].copy()]\n",
    "\n",
    "        words = self.chain(afx)\n",
    "        for x in words: scores.append(self.rntp[x] * self.re_arr['wgts'])\n",
    "        if afx[0] == '_': scores.append(self.re_arr['pd3'])\n",
    "        else: scores.append(self.re_arr['sd3'])\n",
    "        hold = [*self.re_arr['null'].copy()]\n",
    "        for i in range(1, len(scores)-1): hold.append((scores[i+1]-scores[i])+(scores[i-1]-scores[i]))\n",
    "\n",
    "        if afx[0] == '_':\n",
    "            if len(afx) > over_length: hold.extend([self.re_arr['dlpd3'], self.re_arr['dlpd3']])\n",
    "            else: hold.extend([self.re_arr['dpd3'], self.re_arr['dpd3']])\n",
    "        elif afx[-1] == '_':\n",
    "            if len(afx) > over_length: hold.extend([self.re_arr['dsd3'], self.re_arr['dsd3']])\n",
    "            else: hold.extend([self.re_arr['dsd3'], self.re_arr['dsd3']])\n",
    "\n",
    "        out = []\n",
    "        for i in range(2, len(hold)-3):\n",
    "            u1, d1, md = hold[i-1].copy(), hold[i+1].copy(), hold[i]\n",
    "            if self.dbg: print(words[i-2], (md-u1).mean(), (md-d1).mean())\n",
    "            if (md-u1).mean() > bridge_coeff or (md-d1).mean() > bridge_coeff:\n",
    "                u2, d2 = hold[i-2].copy(), hold[i+2].copy()\n",
    "                u2[u2 > u1] *= 0\n",
    "                u1[u1 > hold[i-2]] *= 0\n",
    "                d2[d2 > d1] *= 0\n",
    "                d1[d1 > hold[i+2]] *= 0\n",
    "                u1 = u1 + u2\n",
    "                d1 = d1 + d2\n",
    "            out.append((md-u1)+(md-d1))\n",
    "\n",
    "        if self.dbg:\n",
    "            for i, x in enumerate(out): print(words[i], '\\n', self.remean(x).mean(), '\\n', x)\n",
    "        return [(words[i], self.remean(x).mean()) for i, x in enumerate(out)]\n",
    "\n",
    "    def pulld_relent(self, afx: str, depth: int=1) -> np.ndarray:\n",
    "        #Returns the mean relative entropy of all child nodes of the input affix\n",
    "        hold = [afx]\n",
    "        while depth > 0:\n",
    "            grp = []\n",
    "            while hold:\n",
    "                tmp = self.pulld(hold.pop(), 0)\n",
    "                if tmp:\n",
    "                    for y in tmp:\n",
    "                        grp.append(y)\n",
    "            hold.extend(grp)\n",
    "            depth -= 1\n",
    "        if hold:\n",
    "            hold = np.mean([self.rntp[x] for x in hold], axis=0)\n",
    "            return np.array([*[np.mean(x) for x in hold], *[np.mean(x) for x in hold.T]])\n",
    "        else: return np.array([0]*6)\n",
    "\n",
    "    def graph_relent(self, afx: str):\n",
    "        words = self.chain(afx)\n",
    "        _, ax = plt.subplots(figsize=(16, 10))\n",
    "        yvars = [[*[np.mean(x) for x in self.rntp[w]], *[np.mean(x) for x in self.rntp[w].T], *[np.mean(x) for x in self.drntp[w]], *[np.mean(x) for x in self.drntp[w].T]] for w in words]\n",
    "\n",
    "        plt.xticks(range(len(yvars)), words)\n",
    "        ax.plot(range(len(yvars)), yvars)\n",
    "        ax.legend(['arnd', 'dir', 'exct', 'wnd3', 'wnd2', 'wnd1', 'darnd', 'ddir', 'dexct', 'dwnd3', 'dwnd2', 'dwnd1'])\n",
    "        plt.show()\n",
    "\n",
    "    def find_sub_chain(self, word, afxl=None, sub_depth=0):\n",
    "        if not afxl: afxl = self.verif\n",
    "        fout = set()\n",
    "        #Create list with word and all affixes found in word\n",
    "        targets = [(word, sorted([x for x in afxl if x in word]), [], [])]\n",
    "        while targets:\n",
    "            word, found_afxs, rem, rafxs = targets.pop()\n",
    "            is_match = False\n",
    "            while found_afxs:\n",
    "                #For every affix thats found in a word\n",
    "                afx = found_afxs.pop()\n",
    "                if len(afx) / len(word) > 0.6: continue\n",
    "                sub = self.gsub(word, afx)\n",
    "                if sub:\n",
    "                    is_match = True\n",
    "                    rem.append(word)\n",
    "                    rafxs.append(afx)\n",
    "                    #If there are still affixes left add a new group to targets\n",
    "                    #Allows continuation if word reaches an early dead end\n",
    "                    if found_afxs and any(y in sub for y in found_afxs): targets.append((sub, found_afxs.copy(), rem.copy(), rafxs.copy()))\n",
    "                    found_afxs = sorted([x for x in afxl if x in sub])\n",
    "                    word = sub\n",
    "                #Clean duplicate targets\n",
    "                for x in targets[::-1]:\n",
    "                    if word == x[0] and found_afxs == x[1]: targets.remove(x)\n",
    "                if len(word) < 7: break\n",
    "            #Once no more affixes are found in a word add it to the outputs if atleast 1 matcheed\n",
    "            #If word is still long and no match, use double sub\n",
    "            if is_match: fout.add((tuple(rafxs), (*rem, word)))\n",
    "            elif len(word) > 7 and dsub:\n",
    "                dsub = self.g2sub(word, afxl, depth=sub_depth)\n",
    "                if dsub:\n",
    "                    for k, ds in enumerate(dsub):\n",
    "                        dbl_found = sorted([x for x in afxl if x in ds[0]])\n",
    "                        rcp, rafc = rem.copy(), rafxs.copy()\n",
    "                        rcp.extend(ds[1])\n",
    "                        rafc.extend(ds[2])\n",
    "                        if dbl_found: targets.append((ds[0], dbl_found, rcp, rafc))\n",
    "                        else: fout.add((tuple(rafc), (*rcp, ds[0])))\n",
    "        return tuple(fout)\n",
    "\n",
    "    def g2sub(self, word, afxl=None, depth=1):\n",
    "        if not afxl: afxl = self.verif\n",
    "        queue = {(word, x, (), ()) for x in afxl if x in word}\n",
    "        dupe_key = set()\n",
    "        out = []\n",
    "        while depth > 0 and queue:\n",
    "            newq = set()\n",
    "            for x in queue:\n",
    "                step = self.gsub(x[0], x[1], fltr=False)\n",
    "                if step:\n",
    "                    for y in step:\n",
    "                        for z in afxl:\n",
    "                            if z in y:\n",
    "                                newq.add((y, z, (*x[2], x[0]), (*x[3], x[1])))\n",
    "            queue = newq.copy()\n",
    "            depth -= 1\n",
    "            for x in queue:\n",
    "                g = self.gsub(x[0], x[1])\n",
    "                if g:\n",
    "                    if (g, tuple(sorted((x[0], x[1], *x[2], *x[3])))) not in dupe_key:\n",
    "                        out.append((g, (*x[2], x[0]), (*x[3], x[1])))\n",
    "                        dupe_key.add((g, tuple(sorted((x[0], x[1], *x[2], *x[3])))))\n",
    "        if out: return out\n",
    "\n",
    "    def e2gsub(self, word, it_mx=2, bridges=False):\n",
    "        stg = 0\n",
    "        bkd = {word: {'afx': [], 'reps': [], 'ub': [], 'chk': False, 'bchk': False}}\n",
    "        while stg <= it_mx:\n",
    "            new = []\n",
    "            for w in bkd:\n",
    "                if not bkd[w]['chk']:\n",
    "                    for nx in [x for x in self.verif if x in w]:\n",
    "                        rep = w.replace(nx, '_')\n",
    "                        bkd[w]['afx'].append(nx)\n",
    "                        bkd[w]['reps'].append(rep)\n",
    "                        if rep not in bkd: new.append((rep, bkd[w]['afx'].copy(), bkd[w]['reps'].copy(), bkd[w]['ub'].copy()))\n",
    "                    else: bkd[w]['chk'] = True\n",
    "                elif bridges and not bkd[w]['bchk']:\n",
    "                    b1 = [*[(w.replace(f'_{b}', '_'), f'_{b}') for b in self.ldct['bridges'] if w.startswith(f'_{b}')], \n",
    "                        *[(w.replace(f'{b}_', '_'), f'{b}_') for b in self.ldct['bridges'] if w.endswith(f'{b}_')]]\n",
    "                    b2 = [b for b in b1 if any(bz in b[0] for bz in self.verif)]\n",
    "                    for b in b2:\n",
    "                        if b[0] not in new and b[0] not in bkd:\n",
    "                            obr = [q for q in bkd[w]['ub']]\n",
    "                            new.append((b[0], [z for z in bkd[w]['afx']], [z for z in bkd[w]['reps']], [b[1], *obr]))\n",
    "                    bkd[w]['bchk'] = True\n",
    "            for nw in new:\n",
    "                if nw[0] not in bkd:\n",
    "                    bkd[nw[0]] = {'afx': nw[1], 'reps': nw[2], 'ub': nw[3], 'chk': False, 'bchk': False}\n",
    "            stg += 1\n",
    "        bkd.pop(word)\n",
    "        kl = [(k, 50000) if k in self.roots else (k, self.full_scores[k]) for k in bkd.keys() if k in self.full_scores]\n",
    "        if kl:\n",
    "            ok = sorted(kl, key=lambda x: x[1])[-1][0]\n",
    "            if not bridges: return (ok, bkd[ok]['reps'][:-1], bkd[ok]['afx'][:-1])\n",
    "            else: return (ok, bkd[ok]['reps'][:-1], bkd[ok]['afx'][:-1], bkd[ok]['ub'])\n",
    "\n",
    "    def assign_search_dict(self, words: Container):\n",
    "        self.default_search = words\n",
    "\n",
    "    def load(self, id: int=0) -> None:\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\{id}', 'rb') as f:\n",
    "            self.wlst, self.afx, self.cleared, self.final, self.dsts, self.rntp, self.drntp, self.re_arr, self.full_scores = load(f)\n",
    "\n",
    "    def save(self, id: int=0) -> None:\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\{id}', 'wb') as f:\n",
    "            dump((self.wlst, self.afx, self.cleared, self.final, self.dsts, self.rntp, self.drntp, self.re_arr, self.full_scores), f)\n",
    "\n",
    "class Word:\n",
    "    __slots__ = ('w', 'all', 'core', 'forms', 'mods', 'aliases', 'root', 'plur', 'pres', 'pprg', 'past')\n",
    "    \"\"\"\n",
    "    core: words that use this word as a root\n",
    "    mods: words that use this word to modify another\n",
    "    aliases: alternate names that are not extensible. nicknames, acronyms, foreign languages\n",
    "    forms: conjugations and alternate spellings of the same stem\n",
    "    \"\"\"\n",
    "    def __init__(self, word):\n",
    "        self.w = word\n",
    "        self.all = (word,)\n",
    "        self.core = ()\n",
    "        self.mods = ()\n",
    "        self.forms = ()\n",
    "        self.aliases = ()\n",
    "        self.root = False\n",
    "        self.plur = None\n",
    "        self.pres = None\n",
    "        self.pprg = None\n",
    "        self.past = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if len(self.all) == 1: return self.w\n",
    "        else: return f'{self.all[::-1]}'\n",
    "\n",
    "    def __str__(self):\n",
    "        if len(self.all) == 1: return self.w\n",
    "        else:\n",
    "            ostr = f'{self.all[::-1]}'\n",
    "            for x in [x for x in self.__slots__[2:] if getattr(self, x)]:\n",
    "                if x in ('plur', 'pres', 'pprg', 'past'): ostr += f'\\n{\"    \" * (1+wbs.bt[self.w].index(self.w))} {x}: {getattr(self, x)}'\n",
    "                else: ostr += f',  {x}: {getattr(self, x)}'\n",
    "            return ostr\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        return getattr(self, k)\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        setattr(self, k, v)\n",
    "\n",
    "    def adg(self, k, v):\n",
    "        if v:\n",
    "            attr = getattr(self, k)\n",
    "            if isinstance(v, (str, Word)):\n",
    "                if isinstance(attr, (str, Word)): setattr(self, k, (v, attr))\n",
    "                elif not attr or len(attr) == 0: setattr(self, k, (v,))\n",
    "                else: setattr(self, k, (v, *attr))\n",
    "            else:\n",
    "                if isinstance(attr, (str, Word)): setattr(self, k, (*v, attr))\n",
    "                elif not attr or len(attr) == 0: setattr(self, k, v)\n",
    "                else: setattr(self, k, (*v, *attr))\n",
    "\n",
    "class Lexicon:\n",
    "    #('', ''), ('', ''), \n",
    "    dbl = ('b', 'c', 'd', 'f', 'g', 'l', 'm', 'n', 'p', 'r', 's', 't', 'z')\n",
    "    with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\conjunctions', 'rt') as f:\n",
    "        cnj = [x.strip() for x in f.readlines()]\n",
    "    with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\pronouns', 'rt') as f:\n",
    "        prn = [x.strip() for x in f.readlines()]\n",
    "    pos = 'nvadreci'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bases)\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        if k in self.bases: return self.bases[k]\n",
    "        elif k in self.bt: return self.bases[self.bt[k][0]]\n",
    "        else: raise KeyError\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        if k not in self.bases: \n",
    "            self.bases[k] = Word(v)\n",
    "            self.bt[k] = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.bases.__repr__()\n",
    "\n",
    "    def __init__(self, wlst):\n",
    "        self.bt = {x: None for x in wlst}\n",
    "        self.bases = {x: Word(x) for x in wlst}\n",
    "        self.dbv = ['af', 'ag', 'ap', 'ar', 'as', 'at', 'ef', 'ir', 'il', 'oc', 'of', 'op', 'suc', 'suf', 'sug', 'sup', 'sum', 'syl']\n",
    "        self.cut = ['absc', 'abst', 'ed', 'eg', 'el', 'em', 'ev', 'er', 'ep', 'emb', 'emp', 'ell', 'imb', 'imm', 'imp', 'illu', 'sys', 'sus']\n",
    "\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\affixes', 'rt') as f:\n",
    "            self.mpx = [x.strip() for x in f.readlines()]\n",
    "        self.affixes = {x: Word(x) for x in self.mpx}\n",
    "        self.mpx = sorted([x.strip('_') for x in self.mpx if x[0] == '_'], key=lambda x: len(x))[::-1]\n",
    "        self.xmpx = {x: [z for z in self.mpx if z.startswith(x) and z != x] for x in self.mpx}\n",
    "\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\pre_ex', 'rt') as f:\n",
    "            for x in f.readlines():\n",
    "                x = x.strip().split()\n",
    "                for y in x[1:]:\n",
    "                    self.xmpx[x[0]].append(y)\n",
    "\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\afx_groups', 'rt') as f:\n",
    "            for x in f.readlines():\n",
    "                x = x.strip().split()\n",
    "                for y in x[1:]:\n",
    "                    self.merge_affix(x[0], y)\n",
    "\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\alt_rpls', 'rt') as f:\n",
    "            for x in f.readlines():\n",
    "                x = x.strip().split()\n",
    "                self.update(x[0], x[1], cat='aliases')\n",
    "\n",
    "\n",
    "    def update(self, sink: str, src: str, afx='', cat='', remove=True):\n",
    "        if sink not in self.bases: sink = self.__getitem__(sink).w\n",
    "        self.bases[sink].adg('all', self.bases[src].all)\n",
    "\n",
    "        if cat == 'aliases':\n",
    "            self.bases[sink].adg('aliases', self.bases[src].all)\n",
    "        elif cat == 'Irg':\n",
    "            self.bases[sink].adg(afx, src)\n",
    "            self.bases[sink].adg('all', src)\n",
    "            return\n",
    "        else:\n",
    "            if afx and afx != 'Irg':\n",
    "                if isinstance(afx, (list, tuple)):\n",
    "                    for x in afx:\n",
    "                        if x == 'Irg': continue\n",
    "                        if '_' in x:\n",
    "                            self.affixes[x].adg('mods', src)\n",
    "                            self.affixes[x].adg('all', src)\n",
    "                        else:\n",
    "                            self.bases[x].adg('mods', src)\n",
    "                            self.bases[x].adg('all', src)\n",
    "                else:\n",
    "                    if '_' in afx:\n",
    "                        self.affixes[afx].adg('mods', src)\n",
    "                        self.affixes[afx].adg('all', src)\n",
    "                    else:\n",
    "                        self.bases[afx].adg('mods', src)\n",
    "                        self.bases[afx].adg('all', src)\n",
    "                    \n",
    "            self.bases[sink].adg('core', self.bases[src].core)\n",
    "            self.bases[sink].adg('mods', self.bases[src].mods)\n",
    "            self.bases[sink].adg('forms', self.bases[src].forms)\n",
    "            self.bases[sink].adg('aliases', self.bases[src].aliases)\n",
    "\n",
    "            if cat:\n",
    "                if cat in ('plur', 'pres', 'pprg', 'past'):\n",
    "                    if self.bases[sink][cat]:\n",
    "                        if len(self.bases[src]) == 0: self.bases[sink].adg(cat, src)\n",
    "                        else: self.bases[sink].adg(cat, self.bases[src])\n",
    "                    else:\n",
    "                        if len(self.bases[src]) == 0: self.bases[sink][cat] = src\n",
    "                        else: self.bases[sink][cat] = self.bases[src]\n",
    "                else: self.bases[sink].adg(cat, src)\n",
    "\n",
    "        if remove and src in self.bases: self.bases.pop(src)\n",
    "        if cat == 'aliases': self.bt.pop(src)\n",
    "        else: self.update_track(sink, src)\n",
    "\n",
    "    def update_track(self, sink, src):\n",
    "        if self.bt[sink]: pack = [*self.bt[sink]]\n",
    "        else: pack = [sink]\n",
    "        if self.bt[src]: pack.extend([x for x in self.bt[src] if x not in pack])\n",
    "        else: pack.append(src)\n",
    "        for x in pack: self.bt[x] = tuple(pack)\n",
    "\n",
    "    def rcsfx(self, word, al, r='', t='d', vd='', pref=False) -> bool | str:\n",
    "        if len(word) < al+2: return False\n",
    "        if vd:\n",
    "            if isinstance(vd, tuple):\n",
    "                if word[vd[0]:(vd[0]+len(vd[1]) if vd[0]+len(vd[1]) < 0 else None)] != vd[1]: return False\n",
    "            elif isinstance(vd, dict):\n",
    "                for k in vd:\n",
    "                    if word[-(al+k)] not in vd[k]: return False\n",
    "\n",
    "        if len(word) > 5 and pref:\n",
    "            pword = self.rcpfx(word)\n",
    "            if pword: words = [(word, ''), pword]\n",
    "            else: words = [(word, '')]\n",
    "        else: words = [word]\n",
    "\n",
    "        rls = []\n",
    "        for wx in words:\n",
    "            if pref:\n",
    "                if isinstance(wx, tuple):\n",
    "                    wx, pre = wx\n",
    "                else: pre = ''\n",
    "            if t == 'd':\n",
    "                rep = f'{wx[:-al]}{r}'\n",
    "            elif t == 'dbl':\n",
    "                if len(wx) < (al+3): continue\n",
    "                if wx[-(al+1)] != wx[-(al+2)]: continue\n",
    "                if not vd: vd = {1: 'bcdfglmnprstz'}\n",
    "                elif isinstance(vd, tuple):\n",
    "                    if wx[vd[0]:(vd[0]+len(vd[1]) if vd[0]+len(vd[1]) < 0 else None)] != vd[1]: continue\n",
    "                elif isinstance(vd, dict):\n",
    "                    for k in vd:\n",
    "                        if wx[-(al+k)] not in vd[k]: continue\n",
    "                rep = f'{wx[:-(al+1)]}{r}'\n",
    "            elif t == 'iy':\n",
    "                if wx[-al] != 'i': continue\n",
    "                rep = f'{wx[:-al]}y'\n",
    "            elif t == 'm':\n",
    "                for i in range(1, al+1):\n",
    "                    if wx[:-i] not in self.bases and wx[:-i] not in self.bt: continue\n",
    "                    rep = wx[:-i]\n",
    "                    break\n",
    "                else: continue\n",
    "\n",
    "            if len(rep) > 2 and rep in self.bases or (rep in self.bt and self.bt[rep]): return (rep, pre) if pref else rep\n",
    "            elif pref and len(rep) > 4: rls.append((rep, pre))\n",
    "\n",
    "        if pref:\n",
    "            for x in rls:\n",
    "                rep = self.rcpfx(x[0])\n",
    "                if rep and len(rep[0]) > 3: return (rep[0], x[1], rep[1])\n",
    "\n",
    "    def rcpfx(self, word) -> str:\n",
    "        mtchs = sorted([\n",
    "            y for y in self.mpx\\\n",
    "            if word.startswith(y)\\\n",
    "            and len(word) - len(y) > 2\\\n",
    "            and all(not word.startswith(z) for z in self.xmpx[y])\\\n",
    "            and (True if ((y in self.dbv and word[len(y)] == y[-1]) or y not in self.dbv) else False)], key=lambda x: len(x))[::-1]\n",
    "        for x in mtchs:\n",
    "            if x in self.cut: rep = word[len(x)-1:]\n",
    "            else: rep = word[len(x):]\n",
    "            if rep in self.bases or (rep in self.bt and self.bt[rep]): return (rep, f'_{x}')\n",
    "            elif x in ('ex'):\n",
    "                rep = f's{rep}'\n",
    "                if rep in self.bases or (rep in self.bt and self.bt[rep]): return (rep, f'_{x}')\n",
    "\n",
    "    def rform(self, word, form):\n",
    "        if form in self.bases or form in self.bt:\n",
    "            self.update(word, form, cat='forms')\n",
    "        else:\n",
    "            self.bases[word].adg('forms', form)\n",
    "            self.bases[word].adg('all', form)\n",
    "            if self.bt[word]: pack = [*self.bt[word], form]\n",
    "            else: pack = [word, form]\n",
    "            for x in pack: self.bt[x] = tuple(pack)\n",
    "\n",
    "    def merge_affix(self, sink, src):\n",
    "        self.affixes[sink].adg('all', self.affixes[src].all)\n",
    "        self.affixes[sink].adg('core', self.affixes[src].core)\n",
    "        self.affixes[sink].adg('mods', self.affixes[src].mods)\n",
    "        self.affixes[sink].adg('forms', self.affixes[src].forms)\n",
    "        self.affixes[sink].adg('aliases', self.affixes[src].aliases)\n",
    "        self.affixes[sink].adg('forms', src)\n",
    "        self.affixes[src] = self.affixes[sink]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def homo_parse(self):\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\homos', 'rt') as f:\n",
    "            homos = [x.strip().split() for x in f.readlines()]\n",
    "        for x in homos:\n",
    "            if x[0] == 'singular':\n",
    "                self.update(x[2], x[3], x[1], 'Irg')\n",
    "                if len(x) > 4:\n",
    "                    for y in x[4:]:\n",
    "                        self.update(x[2], y, x[1], 'Irg')\n",
    "            elif x[0] == 'mod':\n",
    "                for z in range(2, len(x), 2):\n",
    "                    self.update(x[1], x[z+1], x[z], 'past')\n",
    "\n",
    "    def irg_parse(self):\n",
    "        #('', ''), \n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\irg_rpls', 'rt') as f:\n",
    "            rpls = [x.strip().split() for x in f.readlines()]\n",
    "        #Wind Wound\n",
    "        for x in rpls:\n",
    "            for y in x[1:]:\n",
    "                self.update(x[0], y, 'Irg', 'past')\n",
    "\n",
    "    def unq_parse(self, pref=False):\n",
    "        #ed\n",
    "        sl = len(self.bases)\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\', 'rt') as f:\n",
    "            rpls = [x.strip().split() for x in f.readlines()]\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\', 'rt') as f:\n",
    "            igls = [x.strip() for x in f.readlines()]\n",
    "        if not pref:\n",
    "            for x in rpls:\n",
    "                self.update(x[0], x[1], cat='core')\n",
    "        fls = ''.split()\n",
    "\n",
    "        al = 2\n",
    "        for x in [x for x in self.bases if len(x) > 3+al and x.endswith('') and (y not in x for y in fls) and x not in igls]:\n",
    "            mx = False\n",
    "            \n",
    "            if x.endswith('') and (mx := self.rcsfx(x, al, pref=pref)): pass\n",
    "            elif x.endswith('') and (mx := self.rcsfx(x, al, pref=pref)): pass\n",
    "\n",
    "            if mx: self.update(self.__getitem__(mx), x, x[-2:], 'core')\n",
    "                \n",
    "        print(f'{sl - len(self.bases)} items combined for plurals\\n{len(self.bases)} remaining')\n",
    "\n",
    "    def pre_parse(self, pref=False):\n",
    "        #ed\n",
    "        sl = len(self.bases)\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\pre_rpls', 'rt') as f:\n",
    "            rpls = [x.strip().split() for x in f.readlines()]\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\pre_igls', 'rt') as f:\n",
    "            igls = [x.strip() for x in f.readlines()]\n",
    "        if not pref:\n",
    "            for x in rpls:\n",
    "                self.update(x[0], x[1], x[2], cat='core')\n",
    "        fls = ''.split()\n",
    "\n",
    "        for x in [x for x in self.bases if any(x.startswith(y) for y in self.mpx)]:\n",
    "            mx = self.rcpfx(x)\n",
    "            if mx and len(mx[0]) > 3:\n",
    "                self.update(self.__getitem__(mx[0]).w, x, mx[1], 'core')\n",
    "        print(f'{sl - len(self.bases)} items combined for prefixes\\n{len(self.bases)} remaining')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def pl_parse(self, pref=False):\n",
    "        #s, ia, a\n",
    "        sl = len(self.bases)\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\pl_rpls', 'rt') as f:\n",
    "            rpls = [x.strip().split() for x in f.readlines()]\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\pl_igls', 'rt') as f:\n",
    "            igls = [x.strip() for x in f.readlines()]\n",
    "        if not pref:\n",
    "            for x in rpls:\n",
    "                self.update(x[0], x[1], x[2], 'plur')\n",
    "        fls = 'series species'.split()\n",
    "\n",
    "        for x in [x for x in self.bases if len(x) > 3 and x.endswith('s') and (y not in x for y in fls) and x not in igls]:\n",
    "            mx = False\n",
    "            \n",
    "            if x.endswith('ies') and (mx := self.rcsfx(x, 3, t='iy', pref=pref)): afx = 'es_'\n",
    "            elif x.endswith('s') and x[-2] != 's' and (mx := self.rcsfx(x, 1, pref=pref)): afx = 's_'\n",
    "            elif x.endswith('es') and (mx := self.rcsfx(x, 2, vd={1: 'shoxz'}, pref=pref)): afx = 'es_'\n",
    "            elif x.endswith('es') and (mx := self.rcsfx(x, 2, t='dbl', pref=pref)): afx = 'es_'\n",
    "            elif x.endswith('ves') and (mx := self.rcsfx(x, 3, r='f', pref=pref)): afx = 'ves_'\n",
    "            elif x.endswith('ices') and ((mx := self.rcsfx(x, 4, r='ix', pref=pref)) or (mx := self.rcsfx(x, 4, r='ex', pref=pref))): afx = 'ices_'\n",
    "            elif x.endswith('es') and (mx := self.rcsfx(x, 2, r='is', pref=pref)): afx = 'es_'\n",
    "\n",
    "            if mx:\n",
    "                if pref: self.update(self.__getitem__(mx[0]).w, x, (afx, *[z for z in mx[1:] if z]), 'plur')\n",
    "                else: self.update(self.__getitem__(mx).w, x, afx, 'plur')\n",
    "\n",
    "        for x in [x for x in self.bases if len(x) > 3 and (x[-1] in 'aeix') and x not in igls]:\n",
    "            mx = False\n",
    "\n",
    "            if x.endswith('ia') and (mx := self.rcsfx(x, 1, r='um', pref=pref)): afx = 'Irg'\n",
    "            elif x.endswith('i') and (mx := self.rcsfx(x, 1, r='us', pref=pref)): afx = 'Irg'\n",
    "            elif x.endswith('eaux') and (mx := self.rcsfx(x, 1, pref=pref)): afx = 'Irg'\n",
    "            elif x.endswith('a') and (mx := self.rcsfx(x, 1, r='on', pref=pref)): afx = 'Irg'\n",
    "            elif x.endswith('ae') and (mx := self.rcsfx(x, 1, pref=pref)): afx = 'Irg'\n",
    "\n",
    "            if mx:\n",
    "                if pref: self.update(self.__getitem__(mx[0]).w, x, (afx, *[z for z in mx[1:] if z]), 'plur')\n",
    "                else: self.update(self.__getitem__(mx).w, x, afx, 'plur')\n",
    "\n",
    "        print(f'{sl - len(self.bases)} items combined for plurals\\n{len(self.bases)} remaining')\n",
    "\n",
    "    def prpt_parse(self, pref=False):\n",
    "        #ing\n",
    "        sl = len(self.bases)\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\prpt_rpls', 'rt') as f:\n",
    "            rpls = [x.strip().split() for x in f.readlines()]\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\prpt_igls', 'rt') as f:\n",
    "            igls = [x.strip() for x in f.readlines()]\n",
    "        if not pref:\n",
    "            for x in rpls:\n",
    "                self.update(x[0], x[1], x[2], 'pprg')\n",
    "        \n",
    "        for x in [x for x in self.bases if len(x) > 5 and x.endswith('ing') and x not in igls]:\n",
    "            mx = False\n",
    "\n",
    "            if (mx := self.rcsfx(x, 3, t='dbl', pref=pref)): pass\n",
    "            elif (mx := self.rcsfx(x, 3, r='e', pref=pref)): pass\n",
    "            elif (mx := self.rcsfx(x, 3, pref=pref)): pass\n",
    "            elif (mx := self.rcsfx(x, 4, vd={0: 'k', 1: 'c'}, pref=pref)): pass\n",
    "            if mx:\n",
    "                if pref: self.update(self.__getitem__(mx[0]).w, x, ('ing_', *[z for z in mx[1:] if z]), 'pprg')\n",
    "                else: self.update(self.__getitem__(mx).w, x, 'ing_', 'pprg')\n",
    "\n",
    "        print(f'{sl - len(self.bases)} items combined for present progressives\\n{len(self.bases)} remaining')\n",
    "\n",
    "    def pt_parse(self, pref=False):\n",
    "        #ed\n",
    "        sl = len(self.bases)\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\pt_rpls', 'rt') as f:\n",
    "            rpls = [x.strip().split() for x in f.readlines()]\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\pt_igls', 'rt') as f:\n",
    "            igls = [x.strip() for x in f.readlines()]\n",
    "        if not pref:\n",
    "            for x in rpls:\n",
    "                self.update(x[0], x[1], x[2], 'past')\n",
    "        \n",
    "        rmls = []\n",
    "        for x in [x for x in self.bases if len(x) > 4 and x.endswith('ed') and x not in igls]:\n",
    "            mx = False\n",
    "\n",
    "            if x.endswith('ied') and (mx := self.rcsfx(x, 3, t='iy', pref=pref)): afx = 'ed_'\n",
    "            elif (mx := self.rcsfx(x, 1, pref=pref)): afx = 'ed_'\n",
    "            elif (mx := self.rcsfx(x, 2, pref=pref)): afx = 'ed_'\n",
    "            elif (mx := self.rcsfx(x, 2, t='dbl', pref=pref)): afx = 'ed_'\n",
    "            elif (mx := self.rcsfx(x, 3, vd=(-4, 'ck'), pref=pref)): afx = 'ed_'\n",
    "            if mx:\n",
    "                if pref: self.update(self.__getitem__(mx[0]).w, x, (afx, *[z for z in mx[1:] if z]), 'past')\n",
    "                else: self.update(self.__getitem__(mx).w, x, afx, 'past')\n",
    "        print(f'{sl - len(self.bases)} items combined for past \\n{len(self.bases)} remaining')\n",
    "\n",
    "    def adjv_parse(self, pref=False):\n",
    "        #er/est | y\n",
    "        sl = len(self.bases)\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\adj_rpls', 'rt') as f:\n",
    "            rpls = [x.strip().split() for x in f.readlines()]\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\adj_igls', 'rt') as f:\n",
    "            igls = [x.strip() for x in f.readlines()]\n",
    "        fls = 'logy berry play copy body away shy fly day'.split()\n",
    "        if not pref:\n",
    "            for x in rpls:\n",
    "                self.update(x[0], x[1], x[2], 'core')\n",
    "\n",
    "            sgrp = [(x, f'{x[:-2]}r') for x in [x for x in self.bases if x.endswith('est')] if f'{x[:-2]}r' in self.bases and x not in igls]\n",
    "            for x in sgrp:\n",
    "                if x[0][-4] == 'i': al = 4\n",
    "                else: al = 3\n",
    "                pack = []\n",
    "                for k, y in enumerate(x):\n",
    "                    if (mx := self.rcsfx(y, al-k, pref=pref)): pack.append(mx)\n",
    "                    if (mx := self.rcsfx(y, al-k, r='e', pref=pref)): pack.append(mx)\n",
    "                    if (mx := self.rcsfx(y, al-k, t='iy', pref=pref)): pack.append(mx)\n",
    "                    if (mx := self.rcsfx(y, al-k, t='dbl', pref=pref)): pack.append(mx)\n",
    "                \n",
    "                if pack:\n",
    "                    pack = tuple(set([z for z in pack if pack.count(z) > 1 and z not in igls]))\n",
    "                    if pack:\n",
    "                        if len(pack) > 1:\n",
    "                            if pack[0][-1] == 'y':\n",
    "                                rt = pack[1]\n",
    "                                self.update(rt, pack[0], 'y_', 'core')\n",
    "                            else:\n",
    "                                rt = pack[0]\n",
    "                                self.update(rt, pack[1], 'y_', 'core')\n",
    "                        else: rt = pack[0]\n",
    "                        self.update(rt, x[0], ('est_' if x[0].endswith('est') else 'er_'), 'core')\n",
    "                        self.update(rt, x[1], ('est_' if x[1].endswith('est') else 'er_'), 'core')\n",
    "\n",
    "        for x in [x for x in self.bases if len(x) > 4 and x.endswith('y') and x not in igls and all(not x.endswith(y) for y in fls)]:\n",
    "            mx = False\n",
    "            afx = f'{x[-2:]}_'\n",
    "\n",
    "            if x.endswith('ly') and (mx := self.rcsfx(x, 2, vd={1: 'bcdefghklmnprstwxy'}, pref=pref)): pass\n",
    "            elif x.endswith('ry') and (mx := self.rcsfx(x, 2, vd={1: 'cdeklnt'}, pref=pref)): pass\n",
    "            elif x.endswith('ty') and (mx := self.rcsfx(x, 2, vd={1: 'elx'}, pref=pref)): pass\n",
    "            elif x.endswith('bility') and (mx := self.rcsfx(x, 5, r='le', pref=pref)): pass\n",
    "            elif x.endswith('cy') and (mx := self.rcsfx(x, 2, r='te', vd={1: 'a'}, pref=pref)): pass\n",
    "            elif x.endswith('cy') and (mx := self.rcsfx(x, 2, r='t', vd={1: 'n'}, pref=pref)): pass\n",
    "            elif (mx := self.rcsfx(x, 1, vd={1: 'dfghklmnprstwxz'}, pref=pref)): afx = 'y_'\n",
    "            elif any(x.endswith(y) for y in ('ily', 'ary', 'ory', 'ity', 'ify')) and (mx := self.rcsfx(x, 3, r='e', vd={1: 'bcdgklmnprstvz'}, pref=pref)): pass\n",
    "            elif (mx := self.rcsfx(x, 1, r='e', vd={1: 'bcdgklmnprstvz'}, pref=pref)): afx = 'y_'\n",
    "            elif (mx := self.rcsfx(x, 1, t='dbl', vd={1: 'bdglmnpt'}, pref=pref)): afx = 'y_'\n",
    "            elif (mx := self.rcsfx(x, 2, vd={0: 'r', 1: 'r', 2: 'u'}, pref=pref)): pass\n",
    "            elif x.endswith('ically') and (mx := self.rcsfx(x, 4, pref=pref)): pass\n",
    "            elif (x.endswith('arily') or x.endswith('sily')) and (mx := self.rcsfx(x, 3, r='y', pref=pref)): pass\n",
    "            elif x.endswith('llary') and (mx := self.rcsfx(x, 4, pref=pref)): pass\n",
    "            elif x.endswith('ary') and (mx := self.rcsfx(x, 3, vd={1: 'bdmnrt'}, pref=pref)): pass\n",
    "            elif x.endswith('ily') and (mx := self.rcsfx(x, 3, vd={1: 'dhkmpt'}, pref=pref)): pass\n",
    "            elif x.endswith('ity') and (mx := self.rcsfx(x, 3, vd={1: 'cdelmnrtx'}, pref=pref)): pass\n",
    "            elif x.endswith('ory') and (mx := self.rcsfx(x, 3, vd={1: 'st'}, pref=pref)): pass\n",
    "            elif x.endswith('ily') and (mx := self.rcsfx(x, 3, t='dbl', vd={1: 'ndp'}, pref=pref)): pass\n",
    "            elif x.endswith('ity') and (mx := self.rcsfx(x, 3, t='dbl', vd={1: 'lp'}, pref=pref)): pass\n",
    "            elif x.endswith('ery') and (mx := self.rcsfx(x, 3, t='dbl', vd={1: 'bgln'}, pref=pref)): pass\n",
    "\n",
    "            if mx:\n",
    "                if pref: self.update(self.__getitem__(mx[0]).w, x, (afx, *[z for z in mx[1:] if z]), 'core')\n",
    "                else: self.update(self.__getitem__(mx).w, x, afx, 'core')\n",
    "        print(f'{sl - len(self.bases)} items combined for adjectives \\n{len(self.bases)} remaining')\n",
    "\n",
    "    def mbr_parse(self, pref=False):\n",
    "        #ist/ian, er/or/ee\n",
    "        sl = len(self.bases)\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\mbr_rpls', 'rt') as f:\n",
    "            rpls = [x.strip().split() for x in f.readlines()]\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\mbr_igls', 'rt') as f:\n",
    "            igls = [x.strip() for x in f.readlines()]\n",
    "        fls = 'meter water power flower polar'.split()\n",
    "        if not pref:\n",
    "            for x in rpls:\n",
    "                self.update(x[0], x[1], x[2], 'core')\n",
    "\n",
    "        al = 3\n",
    "        for x in [x for x in self.bases if len(x) > 5 and (x.endswith('ian') or x.endswith('ist')) and x not in igls]:\n",
    "            mx = False\n",
    "\n",
    "            if (mx := self.rcsfx(x, 1, r='m', pref=pref)): afx = 'ist_'\n",
    "            elif (mx := self.rcsfx(x, al, r='y', pref=pref)): afx = f'{x[-3:]}_'\n",
    "            elif x.endswith('scientist') and (mx := self.rcsfx(x, len('scientist'), r='science', pref=pref)): afx = 'ist_'\n",
    "            elif x.endswith('tarian') and (mx := self.rcsfx(x, 5, r='y', pref=pref)): afx = 'ian_'\n",
    "            elif x.endswith('ician') and (mx := self.rcsfx(x, 3, r='s', pref=pref)): afx = 'cian_'\n",
    "            elif x.endswith('ician') and (mx := self.rcsfx(x, 5, pref=pref)): afx = 'cian_'\n",
    "            elif x.endswith('ian') and (mx := self.rcsfx(x, al, t='m', pref=pref)): afx = 'ian_'\n",
    "            elif (mx := self.rcsfx(x, al, pref=pref)): afx = f'{x[-3:]}_'\n",
    "            elif (mx := self.rcsfx(x, al, r='e', pref=pref)): afx = f'{x[-3:]}_'\n",
    "            elif (mx := self.rcsfx(x, al, t='dbl', pref=pref)): afx = f'{x[-3:]}_'\n",
    "            elif (mx := self.rcsfx(x, al, r='ic', pref=pref)): afx = f'{x[-3:]}_'\n",
    "\n",
    "            if mx:\n",
    "                if pref: self.update(self.__getitem__(mx[0]).w, x, (afx, *[z for z in mx[1:] if z]), 'core')\n",
    "                else: self.update(self.__getitem__(mx).w, x, afx, 'core')\n",
    "\n",
    "        for x in [x for x in self.bases if len(x) > 4 and x[-2:] in ('er', 'ee', 'or') and x not in igls and all(y not in x for y in fls)]:\n",
    "            #and x[-3] in 'stlr'\n",
    "            if x.endswith('ster'): al = 4\n",
    "            elif x.endswith('ier'): al = 3\n",
    "            else: al = 2\n",
    "            mx = False\n",
    "            afx = f'{x[-2:]}_'\n",
    "\n",
    "            if (mx := self.rcsfx(x, al, t='m', pref=pref)): pass\n",
    "            elif (mx := self.rcsfx(x, al, r='e', pref=pref)): pass\n",
    "            elif x.endswith('ier') and (mx := self.rcsfx(x, al+1, t='iy', pref=pref)): pass\n",
    "            elif x.endswith('ier') and (mx := self.rcsfx(x, al+1, r='e', pref=pref)): pass\n",
    "            elif x.endswith('eer') and (mx := self.rcsfx(x, al+1, pref=pref)): pass\n",
    "            elif (mx := self.rcsfx(x, al, t='dbl', pref=pref)): pass\n",
    "            elif x.endswith('ster') and (mx := self.rcsfx(x, al+2, pref=pref)): afx = 'ster_'\n",
    "\n",
    "            if mx:\n",
    "                if pref: self.update(self.__getitem__(mx[0]).w, x, (afx, *[z for z in mx[1:] if z]), 'core')\n",
    "                else: self.update(self.__getitem__(mx).w, x, afx, 'core')\n",
    "                \n",
    "        print(f'{sl - len(self.bases)} items combined for membership \\n{len(self.bases)} remaining')\n",
    "\n",
    "    def sfx1_parse(self, pref=False):\n",
    "        sl = len(self.bases)\n",
    "        rpls = [('use', 'usable'), ('note', 'notable'), ('ride', 'ridable'), ('erase', 'erasable'), ('tend', 'tenable')]\n",
    "        igls = 'parable liable capable arable sister'.split()\n",
    "        if not pref:\n",
    "            for x in rpls:\n",
    "                self.update(x[0], x[1], 'able', 'core')\n",
    "\n",
    "        al = 4\n",
    "        for x in [x for x in self.bases if len(x) > 4 and x not in igls and any(x.endswith(t) for t in ('less', 'ness', 'able', 'ible'))]:\n",
    "            mx = False\n",
    "\n",
    "            if (mx := self.rcsfx(x, al, pref=pref)): pass\n",
    "            elif (x.endswith('able') or x.endswith('ible')) and (mx := self.rcsfx(x, al, r='e', pref=pref)): pass\n",
    "            elif x[-5] == 'i' and (mx := self.rcsfx(x, al+1, t='iy', pref=pref)): pass\n",
    "\n",
    "            if mx:\n",
    "                if x.endswith('ness'): afx = 'ness_'\n",
    "                elif x.endswith('ible'): afx = 'able'\n",
    "                else: afx =x[-4:]\n",
    "                if pref: self.update(self.__getitem__(mx[0]).w, x, (afx, *[z for z in mx[1:] if z]), 'core')\n",
    "                else: self.update(self.__getitem__(mx).w, x, afx, 'core')\n",
    "\n",
    "        for tgt in (('woman', 'women'), ('man', 'men')):\n",
    "            al = len(tgt[0])\n",
    "            for x in [x for x in self.bases if len(x) > 5 and x not in igls and (x.endswith(tgt[0]) or x.endswith(tgt[1]))]:\n",
    "                mx = False\n",
    "\n",
    "                if (mx := self.rcsfx(x, al, pref=pref)): pass\n",
    "                if mx:\n",
    "                    if pref: self.update(tgt[0], x, (self.__getitem__(mx[0]).w, *[z for z in mx[1:] if z]), 'core')\n",
    "                    else: self.update(tgt[0], x, self.__getitem__(mx).w, 'core')\n",
    "        print(f'{sl - len(self.bases)} items combined for sfx1 \\n{len(self.bases)} remaining')\n",
    "\n",
    "    def v2d_parse(self, pref=False):\n",
    "        sl = len(self.bases)\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\v2d_rpls', 'rt') as f:\n",
    "            rpls = [x.strip().split() for x in f.readlines()]\n",
    "        with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\v2d_igls', 'rt') as f:\n",
    "            igls = [x.strip() for x in f.readlines()]\n",
    "        if not pref:\n",
    "            for x in rpls:\n",
    "                self.update(x[0], x[1], x[2:], 'core')\n",
    "        fls = 'active drive ceptive ceive dive hive'.split()\n",
    "\n",
    "        al = 3\n",
    "        for x in [x for x in self.bases if len(x) > 3+al and x.endswith('ive') and (y not in x for y in fls) and x not in igls]:\n",
    "            mx = False\n",
    "\n",
    "            if (mx := self.rcsfx(x, al, pref=pref)): pass\n",
    "            elif (mx := self.rcsfx(x, al, pref=pref)): pass\n",
    "            elif (mx := wbs.rcsfx(x, al+1, r='de', vd={0: 's', 1: 'aeiou'}, pref=pref)): pass\n",
    "            elif (mx := wbs.rcsfx(x, al+2, r='e', vd=(-5, 'it'), pref=pref)): pass\n",
    "            elif (mx := wbs.rcsfx(x, al+2, vd=(-5, 'it'), pref=pref)): pass\n",
    "            elif (mx := wbs.rcsfx(x, al+1, r='d', vd={0: 'sn'}, pref=pref)): pass\n",
    "            elif (mx := wbs.rcsfx(x, al, pref=pref)): pass\n",
    "            elif (mx := wbs.rcsfx(x, al, r='e', pref=pref)): pass\n",
    "            elif (mx := wbs.rcsfx(x, al+2, r='e', vd=(-5, 'at'), pref=pref)): pass\n",
    "            elif (mx := wbs.rcsfx(x, al+2, vd=(-5, 'at'), pref=pref)): pass\n",
    "            elif (mx := wbs.rcsfx(x, al+2, r='y', vd=(-5, 'at'), pref=pref)): pass\n",
    "\n",
    "            if mx:\n",
    "                if pref: self.update(self.__getitem__(mx[0]).w, x, ('ive_', *[z for z in mx[1:] if z]), 'core')\n",
    "                else: self.update(self.__getitem__(mx).w, x, 'ive_', 'core')\n",
    "        print(f'{sl - len(self.bases)} items combined for plurals\\n{len(self.bases)} remaining')\n",
    "\n",
    "\n",
    "\n",
    "with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\tests', 'rt') as f:\n",
    "    tests = [x.strip() for x in f.readlines()]\n",
    "\n",
    "a, roots, nfx = setup()\n",
    "a.assign_search_dict(a.bare)\n",
    "wbs = Lexicon(set([x.strip('_') for x in usk_rep(a.wlst)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wbs.homo_parse()\n",
    "wbs.irg_parse()\n",
    "\n",
    "wbs.pl_parse()\n",
    "wbs.prpt_parse()\n",
    "wbs.pt_parse()\n",
    "wbs.adjv_parse()\n",
    "wbs.mbr_parse()\n",
    "wbs.sfx1_parse()\n",
    "\n",
    "wbs.pl_parse(pref=True)\n",
    "wbs.prpt_parse(pref=True)\n",
    "wbs.pt_parse(pref=True)\n",
    "wbs.adjv_parse(pref=True)\n",
    "wbs.mbr_parse(pref=True)\n",
    "wbs.sfx1_parse(pref=True)\n",
    "\n",
    "wbs.pre_parse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in wbs.bases if len(x) > 4 and x[-2:] in ('er', 'ee', 'or') and x[-3] in 'stlr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\base1', 'wb') as f:\n",
    "    dump(wbs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppres = sorted(lrsort(roots, trim=True), key=lambda x: len(x))[::-1]\n",
    "pre = []\n",
    "\n",
    "while ppres:\n",
    "    x = ppres.pop()\n",
    "    tmp = [x]\n",
    "    for y in [z for z in ppres if z.startswith(x) and x != z]:\n",
    "        ppres.remove(y)\n",
    "        tmp.append(y)\n",
    "    pre.append(tmp)\n",
    "    \n",
    "afgen = (x for x in pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\base1', 'rb') as f:\n",
    "    wbs = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\root groups', 'rt') as f:\n",
    "    root_groups = (x.strip().split() for x in f.readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sword = 'bine'\n",
    "wbs.update_bases('bin', sword, 'root', 'form')\n",
    "if sword in wbs.bases: wbs.bases.pop(sword)\n",
    "\n",
    "wbs.rform('ball', 'bol')\n",
    "group.append('category')\n",
    "for x in ['categor']:\n",
    "    group.remove(x)\n",
    "    if x in wbs.bt: print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in group:\n",
    "    if x in wbs.bases: wbs.bases[x].root = True\n",
    "    elif x in wbs.bt: wbs.bases[wbs.bt[x][0]].root = True\n",
    "    else:\n",
    "        wbs.bases[x] = Word(x)\n",
    "        wbs.bases[x].root = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = 'able'\n",
    "group = [x for x in group if x != center]\n",
    "\n",
    "for x in group:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = next(root_groups)\n",
    "cprint(group, [1, 3, 5], hlite=True)\n",
    "tgroup = [x[:-1] if x[-1] in 'aeiou' else x for x in group]\n",
    "c1 = [[y for y in wbs.bases if x in y and all(z not in y for z in tgroup if z != x and len(z) >= len(x))] for x in tgroup]\n",
    "\n",
    "cprint(tgroup, [1, 3, 5], hlite=True)\n",
    "cprint(c1, [1, 3, 5], hlite=group, hl_col=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = [(x, wbs[x].forms, wbs[x].mod) for x in wbs.bases if wbs[x].root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = [(x, wbs[x].forms, wbs[x].mod) for x in wbs.bases if wbs[x].root]\n",
    "\n",
    "for x in hold:\n",
    "    if x[0] in wbs.bases:\n",
    "        wbs.bases[x[0]].root = True\n",
    "    elif x[0] in wbs.bt:\n",
    "        wbs.bases[wbs.bt[x[0]][0]].root = True\n",
    "    else:\n",
    "        wbs.add_word(x[0], root=True)\n",
    "    if x[1]:\n",
    "        for y in x[1]:\n",
    "            if y in wbs.bt:\n",
    "                wbs.update_bases(x[0], (y.w if isinstance(y, Word) else y), 'root', 'forms')\n",
    "                wbs.bases.pop((y.w if isinstance(y, Word) else y))\n",
    "            else:\n",
    "                wbs.rform(x[0], (y.w if isinstance(y, Word) else y))\n",
    "    if x[2]:\n",
    "        for y in x[2]:\n",
    "            wbs.update_bases(x[0], y.w, 'root', 'mod')\n",
    "            wbs.bases.pop(y.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sterms = ['ive']\n",
    "igls = ()\n",
    "al = 3\n",
    "c0 = []\n",
    "c1 = [x for x in wbs.bases if len(x) > 6 and any(x.endswith(y) for y in sterms) and all(y not in x for y in igls)]\n",
    "c1 = [x.strip() for x in rrsort(c1)]\n",
    "cgrp = [list() for _ in range(9)]\n",
    "\n",
    "for x in c1:\n",
    "    tests1 = [\n",
    "        (mx := wbs.rcsfx(x, al+1, r='de', vd={0: 's', 1: 'aeiou'})),\n",
    "        (mx := wbs.rcsfx(x, al+2, r='e', vd=(-5, 'it'))),\n",
    "        (mx := wbs.rcsfx(x, al+2, vd=(-5, 'it'))),\n",
    "        (mx := wbs.rcsfx(x, al+1, r='d', vd={0: 'sn'})),\n",
    "        (mx := wbs.rcsfx(x, al)),\n",
    "        (mx := wbs.rcsfx(x, al, r='e')),\n",
    "        (mx := wbs.rcsfx(x, al+2, r='e', vd=(-5, 'at'))),\n",
    "        (mx := wbs.rcsfx(x, al+2, vd=(-5, 'at'))),\n",
    "        (mx := wbs.rcsfx(x, al+2, r='y', vd=(-5, 'at'))), \n",
    "    ]\n",
    "    for i, y in enumerate(tests1):\n",
    "        if y:\n",
    "            c0.append('-')\n",
    "            cgrp[i].append(y)\n",
    "            for k in range(len(cgrp)):\n",
    "                if k != i:\n",
    "                    cgrp[k].append('_')\n",
    "            break\n",
    "    else:\n",
    "        tests2 = [\n",
    "            (mx := wbs.rcsfx(x, al+1, r='de', vd={0: 's', 1: 'aeiou'}, pref=True)),\n",
    "            (mx := wbs.rcsfx(x, al+2, r='e', vd=(-5, 'it'), pref=True)),\n",
    "            (mx := wbs.rcsfx(x, al+2, vd=(-5, 'it'), pref=True)),\n",
    "            (mx := wbs.rcsfx(x, al+1, r='d', vd={0: 'sn'}, pref=True)),\n",
    "            (mx := wbs.rcsfx(x, al, pref=True)),\n",
    "            (mx := wbs.rcsfx(x, al, r='e', pref=True)),\n",
    "            (mx := wbs.rcsfx(x, al+2, r='e', vd=(-5, 'at'), pref=True)),\n",
    "            (mx := wbs.rcsfx(x, al+2, vd=(-5, 'at'), pref=True)),\n",
    "            (mx := wbs.rcsfx(x, al+2, r='y', vd=(-5, 'at'), pref=True)), \n",
    "        ]\n",
    "        for i, y in enumerate(tests2):\n",
    "            if y:\n",
    "                c0.append('-')\n",
    "                cgrp[i].append(y)\n",
    "                for k in range(len(cgrp)):\n",
    "                    if k != i:\n",
    "                        cgrp[k].append('_')\n",
    "                break\n",
    "        else:\n",
    "            c0.append('X')\n",
    "            for i in range(len(cgrp)): cgrp[i].append('_')\n",
    "\n",
    "print(f'\\t\\t\\t\\t\\t{len([x for x in c0 if x == \"X\"])} / {len(c1)}')\n",
    "cprint([len(c1) - x.count('_') for x in cgrp], [10, 14], col_width=4, halign='r', hlite=True)\n",
    "cprint(\n",
    "    [c0, c1, *cgrp],\n",
    "    [1, 1, 10, 14], \n",
    "    halign={0: 'r', 1: 'l', 2: 'r'},\n",
    "    col_width=4,\n",
    "    hlite=['X', 'ative', 'ade', 'sive'],\n",
    "    hl_col=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = 5\n",
    "c1 = [x for x in wbs.bases if len(x) > 6 and (x.endswith('ess') and x[-4] not in 'ln')]\n",
    "c1 = [x.strip() for x in rrsort(c1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = {i: 0 for i, _ in enumerate(c1)}\n",
    "for i, x in enumerate(c1):\n",
    "    for y in cgrp:\n",
    "        if y[i] != '_': c0[i] += 1\n",
    "c0 = ['X' if x == 0 else '-' for x in c0.values()]\n",
    "\n",
    "print(f'\\t\\t\\t\\t\\t{len([x for x in c0 if x == \"X\"])} / {len(c1)}')\n",
    "#cprint(['ade rep', '-1e rep', '-1 drop', 'ss-t rep', 'd rep', 't rep', 've rep', 'el rep', 'drop'], [10, 14, 18, 22, 26, 30, 34, 38, 42, 50, 54], col_width=4, halign='r', hlite=True)\n",
    "cprint([len(c1) - x.count('_') for x in cgrp], [10, 14], col_width=4, halign='r', hlite=True)\n",
    "\n",
    "cprint(\n",
    "    [c0, c1, *cgrp],\n",
    "    [1, 1, 10, 14], \n",
    "    halign={0: 'r', 1: 'l', 2: 'r'},\n",
    "    col_width=4,\n",
    "    hlite=['ess', 'ress', '_'],\n",
    "    hl_col=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sterms = ['able', 'ible']\n",
    "igls = ()\n",
    "al = 4\n",
    "c0 = []\n",
    "c1 = [x for x in wbs.bases if len(x) > 6 and any(x.endswith(y) for y in sterms) and all(y not in x for y in igls)]\n",
    "c1 = [x.strip() for x in rrsort(c1)]\n",
    "cgrp = [list() for _ in range(9)]\n",
    "\n",
    "\n",
    "\n",
    "for x in c1:\n",
    "    tests1 = [\n",
    "        (mx := wbs.rcsfx(x, al)),\n",
    "        (x.endswith('able') and wbs.rcsfx(x, al, r='e')),\n",
    "        (x[-5] == 'i' and wbs.rcsfx(x, al+1, t='iy')) \n",
    "    ]\n",
    "    for i, y in enumerate(tests1):\n",
    "        if y:\n",
    "            c0.append('-')\n",
    "            cgrp[i].append(y)\n",
    "            for k in range(len(cgrp)):\n",
    "                if k != i:\n",
    "                    cgrp[k].append('_')\n",
    "            break\n",
    "    else:\n",
    "        tests2 = [\n",
    "            (mx := wbs.rcsfx(x, al)),\n",
    "            (x.endswith('able') and wbs.rcsfx(x, al, r='e')),\n",
    "            (x[-5] == 'i' and wbs.rcsfx(x, al+1, t='iy')) \n",
    "        ]\n",
    "        for i, y in enumerate(tests2):\n",
    "            if y:\n",
    "                c0.append('-')\n",
    "                cgrp[i].append(y)\n",
    "                for k in range(len(cgrp)):\n",
    "                    if k != i:\n",
    "                        cgrp[k].append('_')\n",
    "                break\n",
    "        else:\n",
    "            c0.append('X')\n",
    "            for i in range(len(cgrp)): cgrp[i].append('_')\n",
    "\n",
    "print(f'\\t\\t\\t\\t\\t{len([x for x in c0 if x == \"X\"])} / {len(c1)}')\n",
    "cprint([len(c1) - x.count('_') for x in cgrp], [10, 14], col_width=4, halign='r', hlite=True)\n",
    "cprint(\n",
    "    [c0, c1, *cgrp],\n",
    "    [1, 1, 10, 14], \n",
    "    halign={0: 'r', 1: 'l', 2: 'r'},\n",
    "    col_width=4,\n",
    "    hlite=['X', 'ative', 'ade', 'sive'],\n",
    "    hl_col=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppres = sorted(wbs.mpx, key=lambda x: len(x))[::-1]\n",
    "pre = []\n",
    "\n",
    "while ppres:\n",
    "    x = ppres.pop()\n",
    "    tmp = [x]\n",
    "    for y in [z for z in ppres if z.startswith(x) and x != z]:\n",
    "        ppres.remove(y)\n",
    "        tmp.append(y)\n",
    "    pre.append(tmp)\n",
    "    \n",
    "afgen = (x for x in pre)\n",
    "\n",
    "group = next(afgen)\n",
    "hlls, columns, positions, labels, aldct = ['X'], [], [], [], {}\n",
    "pix, aix = 1, 0\n",
    "\n",
    "if len(group) > 1:\n",
    "    pack = [[z.strip() for z in lrsort([y for y in a.bare if y.startswith(x) and all(z not in y for z in group if z != x and len(z) >= len(x))])] for x in group]\n",
    "    hlls.extend(group)\n",
    "else:\n",
    "    pack = [[z.strip() for z in lrsort([x for x in a.bare if x.startswith(group[0])])]]\n",
    "    hlls.append(group[0])\n",
    "for i, p in enumerate(pack):\n",
    "    reps, tally = [], []\n",
    "    for x in p:\n",
    "        r = wbs.rcpfx(x)\n",
    "        if r:\n",
    "            reps.append(r)\n",
    "            tally.append('-')\n",
    "        else:\n",
    "            reps.append('_')\n",
    "            tally.append('X')\n",
    "\n",
    "    columns.extend([tally, p, reps])\n",
    "    positions.extend([pix, pix+1, pix+10])\n",
    "    aldct[aix], aldct[aix+1], aldct[aix+2] = 'l', 'l', 'r'\n",
    "    pix += 10\n",
    "    aix += 3\n",
    "    labels.append(f'{hlls[1+i]}   {len([z for z in tally if z == \"-\"])}/{len(p)}')\n",
    "\n",
    "cprint(labels, [positions[i] for i in range(0, len(positions), 3)], halign='l', col_width=4, hlite=True)\n",
    "cprint(columns, positions, halign=aldct, col_width=4, hlite=hlls)\n",
    "\n",
    "pf = 'irr'\n",
    "lrsort([x for x in a.bare if x.startswith(pf[:-1]) and not x.startswith(pf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bls = [f'_{z}_' for z in wbs.bases]\n",
    "als = {x: tuple([y for y in bls if x in y]) for x in nfx}\n",
    "fcnt = Counter({x[0]: len(x[1]) for x in als.items()})\n",
    "dupe = {x: [y for y in nfx if x in y and x != y] for x in nfx if len([y for y in nfx if x in y and x != y]) > 0}\n",
    "for x in dupe.items():\n",
    "    for y in x[1]:\n",
    "        fcnt[x[0]] -= fcnt[y]\n",
    "\n",
    "sx = [x.strip() for x in rrsort([x for x in nfx if x.endswith('_')])]\n",
    "px = [x.strip() for x in lrsort([x for x in nfx if x.startswith('_')])]\n",
    "als = {x[0]: (tuple([y for y in x[1] if all(z not in y for z in dupe[x[0]])]) if x[0] in dupe else x[1]) for x in als.items()}\n",
    "rgroups = {x: sorted([y for y in roots if x in y and x != y], key=lambda x: len(x))[::-1] for x in roots if len(x) > 2}\n",
    "solos = [x[0] for x in rgroups.items() if len(x[1]) == 0]\n",
    "lgroups = [(x[0], [*x[1], x[0]]) for x in rgroups.items() if len(x[1]) > 3]\n",
    "sgroups = [(x[0], [*x[1], x[0]]) for x in rgroups.items() if 0 < len(x[1]) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfx = [x.strip() for x in rdx_sort(list(set([f'{x}{(9-len(x)) * \" \"}' for x in nfx if x.startswith('_')])))]\n",
    "sfx = [x.strip() for x in rdx_sort(list(set([f'{(9-len(x)) * \" \"}{x}' for x in nfx if x.endswith('_')])), mcd=True)]\n",
    "pfx.extend(sfx)\n",
    "nfx = pfx.copy()\n",
    "roots = [x.strip() for x in rdx_sort(list(set([f'{x}{(9-len(x)) * \" \"}' for x in roots])))]\n",
    "\n",
    "with open(f'C:\\\\Users\\\\BBA\\\\Coding\\\\NLP\\\\Embeddings\\\\data\\\\v0\\\\awork4', 'wb') as f:\n",
    "    dump((roots, nfx), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f5fdee5dc54aebd0e437a010d5f0b24f3c36ee52e7479059de66cc5d4ada99b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
